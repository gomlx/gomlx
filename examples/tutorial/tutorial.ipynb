{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1be69dd2-b703-459e-980c-b91e34784402",
   "metadata": {},
   "source": [
    "<img alt=\"[GoMLX Mascot]\" src=\"gomlx_gopher.jpg\" style=\"height:10em; float:left; margin:1em;\"/>\n",
    "\n",
    "# GoMLX Tutorial \n",
    "\n",
    "If you want just to quickly look at an working example, checkout [examples/cifar/demo/adult.ipynb](https://github.com/gomlx/gomlx/blob/main/examples/cifar/cifar.ipynb), for model trained on the UCI Adult Income dataset. More examples in [examples/](https://github.com/gomlx/gomlx/tree/main/examples) subdirectory. \n",
    "\n",
    "The tutorial won't detail the whole API, but should present all important concepts. Everything else is\n",
    "well documented in godoc (in the code), also available in [pkg.go.dev](https://pkg.go.dev/github.com/gomlx/gomlx#section-readme).\n",
    "\n",
    "The tutorial was written using a Jupyter notebook with [GoNB](https://github.com/janpfeifer/gonb), a kernel for Go that was co-developed with **GoMLX**. It has its own [short tutorial](https://github.com/janpfeifer/gonb/blob/main/examples/tutorial.ipynb) for those interested.\n",
    "\n",
    "If you are seeing this tutorial from [github](https://github.com/gomlx/gomlx/blob/main/examples/tutorial/tutorial.ipynb) snapshop, you won't be able to interact with it. To be able to play with it, try **installing GoMLX**, see its [README Installation section](https://github.com/gomlx/gomlx#installation). The easiest way is to start the pre-generated docker and use the Jupyter notebook there -- this tutorial can be opened from there in an interactive way.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d4a4b5-7d5b-4293-b9d4-88bb00c26abf",
   "metadata": {},
   "source": [
    "## Computation Graphs\n",
    "\n",
    "> [Package `graph` reference documentation](https://pkg.go.dev/github.com/gomlx/gomlx/graph)\n",
    "\n",
    "To do machine learning based on neural networks and gradient descent one of most important\n",
    "requirements is the ability to do mathematical computations (mostly matrix multiplications)\n",
    "fast.\n",
    "\n",
    "GoMLX is built on the concept of building \"computation graphs\", just-in-time compiling them \n",
    "and only then executing them to get the desired results. That means one has to write code\n",
    "that generates other type of code (computation graph) so to say. We do this because then we are\n",
    "able to execute it really fast using [XLA](https://github.com/openxla/xla).\n",
    "\n",
    "For example, let's create a computation graph to sum two values:\n",
    "\n",
    "> **Note**\n",
    "> - If executing this on a notebook, notice the very first cell execution takes a few seconds for Go to fetch the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c58651-25bb-47ab-ab60-6e8a7beeebe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import . \"github.com/gomlx/gomlx/graph\"\n",
    "\n",
    "func SumGraph(a, b *Node) *Node {\n",
    "    return Add(a, b)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631ec80-f864-4743-8124-757f430ed221",
   "metadata": {},
   "source": [
    "> **Note**\n",
    "> \n",
    "> - The `import . \"github.com/gomlx/gomlx/graph\"` import all definitions in computation to the current scope. Usually,\n",
    ">   it's easier to work like this on go files that are going to implement graph building functions.\n",
    "> - Our function is named `SumGraph`: the suffix `Graph` is just a convention, but it helps identifying functions \n",
    ">   that do graph building.\n",
    "> - The type `*Node` represents a node in the computation graph. All graph operations either take a\n",
    ">   `*Graph` object to start with, or a `*Node`, and create new nodes with the corresponding operations.\n",
    ">   So our example will create an `Add` node that will take the nodes pointed by `a` and `b`, build a node \n",
    ">   that represent their summation and then return this `*Node`. \n",
    "> - Every node contains a reference to the `*Graph` it's part of (see [`Node.Graph()`](https://pkg.go.dev/github.com/gomlx/gomlx/graph#Node.Graph)).\n",
    "> - There is a rich set of operations available in GoMLX, see [`Node` documentation](https://pkg.go.dev/github.com/gomlx/gomlx/graph#Node).\n",
    "\n",
    "Ok, but this won't tell us what is 1+1 yet. We need to compile and then execute this graph with\n",
    "some input values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41235bcc-8acb-478a-86ae-bd67f9ab0766",
   "metadata": {},
   "source": [
    "## Executing Graphs with `Exec`\n",
    "\n",
    "[`Exec`](https://pkg.go.dev/github.com/gomlx/gomlx/graph#Exec) is the easiest way to compile and execute computation graphs in GoMLX. \n",
    "To run our `SumGraph` function above we can do:\n",
    "\n",
    "> **Note**: if you have a GPU and this is the very first time you run, NVidia/XLA take a couple of minutes pre-caching \"stuff\" (only happens once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f99a97a-6d64-4cad-9981-43b784d569b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1+1=2\n"
     ]
    }
   ],
   "source": [
    "var manager = NewManager()\n",
    "\n",
    "%%\n",
    "exec := NewExec(manager, SumGraph)  \n",
    "two := exec.Call(1, 1)[0]\n",
    "fmt.Printf(\"1+1=%v\\n\", two.Value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b3b41-b814-4e57-8968-bde0de4dccaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **Note**\n",
    ">\n",
    "> - `%%` is a shortcut for `func main()`: everything after it is put inside a `main` function by **GoNB**, the Go Notebook kernel.\n",
    "> - `NewManager()` creates a `Manager` object, which connects to an accelerator if present. \n",
    ">   Usually one creates one at the beginning of the program and passes it around.\n",
    ">   Here **GoNB** will keep the global variable `manager` available to all cells, so we don't need\n",
    ">   to define it again.\n",
    ">   - The API also offers `BuildManager` method, that allows fine-grained control on which accelerator (or not)\n",
    ">     to use, number of threads, etc.\n",
    "> - The `Exec` object created is associated with a graph building function (`SumGraph` in this case). It lazily \n",
    ">   compiles and executes the compiled computation as needed. Naturally the first time `Call()` is invoked\n",
    ">   it is slow: it has to build the graph and *just-in-time* (JIT) compile it. But the compiled graph afterwards \n",
    ">   is optimized and very fast to execute, which is what we want for machine learning.\n",
    "> - The `Exec.Call(1, 1)` method always returns a slice of results, along with an optional error (ignored here), \n",
    ">   independent of the number of outputs of the Graph building function. That's why we use `[0]` to access the\n",
    ">   first result.\n",
    "> - The results of graph execution are always **tensors** (see section below). They can be converted to Go types\n",
    ">   using `.Value()` method.\n",
    "\n",
    "Something important to understand is that **Graphs have static (fixed) shapes for its inputs and outputs**. What\n",
    "it means is that, for example, if you are going to sum floats instead of ints, `Exec` would have to rebuild the graph\n",
    "to take as input two floats. Or if you want to sum a vector or matrix or ints, or any different `shapes.Shape`.\n",
    "\n",
    "> For a detailed explanation of `Shape` and the associate concepts of Axis, Dimensions and `DType` (the underlying data type), \n",
    "see [package `shapes` documentation](https://pkg.go.dev/github.com/gomlx/gomlx/types/shapes).\n",
    "\n",
    "To exemplify, let's expand our code a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b1a61bf-f902-43db-a2f0-8c8ddf61375d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* building graph for a.shape=(Int64)[] and b.shape=(Int64)[]\n",
      "1+1=2\n",
      "0+0=0\n",
      "1+1=2\n",
      "2+2=4\n",
      "3+3=6\n",
      "4+4=8\n",
      "* building graph for a.shape=(Float64)[] and b.shape=(Float64)[]\n",
      "3.5+1.5=5\n",
      "* building graph for a.shape=(Float32)[3] and b.shape=(Float32)[3]\n",
      "[1.1, 2.2, 3.3] + [10, 10, 10] = [11.1 12.2 13.3]\n"
     ]
    }
   ],
   "source": [
    "import (\n",
    "    \"fmt\"\n",
    "    . \"github.com/gomlx/gomlx/graph\"\n",
    ")\n",
    "\n",
    "func SumGraph(a, b *Node) *Node {\n",
    "    fmt.Printf(\"* building graph for a.shape=%s and b.shape=%s\\n\", a.Shape(), b.Shape())\n",
    "    return Add(a, b)\n",
    "}\n",
    "\n",
    "\n",
    "func main() {\n",
    "    sumExec := NewExec(manager, SumGraph)\n",
    "    two := sumExec.Call(1, 1)[0]\n",
    "    fmt.Printf(\"1+1=%v\\n\", two.Value())\n",
    "\n",
    "    for ii := 0; ii < 5; ii++ {\n",
    "        sumInts := sumExec.Call(ii, ii)[0]\n",
    "        fmt.Printf(\"%d+%d=%v\\n\", ii, ii, sumInts.Value())\n",
    "    }\n",
    "\n",
    "    five := sumExec.Call(3.5, 1.5)[0]\n",
    "    fmt.Printf(\"3.5+1.5=%v\\n\", five.Value())\n",
    "\n",
    "    many := sumExec.Call([]float32{1.1, 2.2, 3.3}, []float32{10, 10, 10})[0]\n",
    "    fmt.Printf(\"[1.1, 2.2, 3.3] + [10, 10, 10] = %v\\n\", many.Value())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1073e-bc4e-4278-a982-758bd5be4200",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">   - Each time a new graph is created, we added a `fmt.Printf` to tell us\n",
    ">     the shape of the graph operands. Notice that `fmt.Printf` is not included in the graph,\n",
    ">     it's only part of the graph building function. We'll see later how to print\n",
    ">     intermediary results in the middle of the execution of the graph.\n",
    ">   - Every `Node` has an associated shape (`shapes.Shape` type). A shape is defined by its underlying data type \n",
    ">     `shapes.DType` and its axes dimensions. For scalars, the shape has zero axes (dimensions). E.g.: `(Int64)[]` represents\n",
    ">     a scalar `int` value, and `(Float32)[3]` represents a vector with 3 `float32` values. More details\n",
    ">     and the list of type supported in the package `gomlx/types`.\n",
    ">   - `Exec` automatically calls `SumGraph` whenever the `Call()` method sees parameters of shapes\n",
    ">     different from it has seen before (there is a cache of pre-compiled graphs kept in memory with limited size).\n",
    "\n",
    "In general the graph operations only work with the same `shapes.DType` (\"data type\"). \n",
    "If they are different, they are reported back with a `panic` (works like an exception, and can be caught) with an error with a full stack-trace in the returned result. \n",
    "\n",
    "Let's create an example with an error to see how this goes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce20082-e817-47a4-ac3e-9eec27a6e3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* building graph for a.shape=(Float64)[] and b.shape=(Int64)[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "panic: operands of AddNode have different dtypes (Float64 and Int64)\n",
      "\n",
      "goroutine 1 [running]:\n",
      "github.com/gomlx/gomlx/types/exceptions.Panicf(...)\n",
      "\t/home/jupyter/Projects/gomlx/types/exceptions/exceptions.go:69\n",
      "github.com/gomlx/gomlx/graph.twoArgsNode(0x33, 0xc0000dc0a0, 0xc0000dc1e0)\n",
      "\t/home/jupyter/Projects/gomlx/graph/node.go:459 +0x193\n",
      "github.com/gomlx/gomlx/graph.Add(...)\n",
      "\t/home/jupyter/Projects/gomlx/graph/node.go:466\n",
      "main.SumGraph(0xc0000dc0a0, 0xc0000dc1e0)\n",
      "\t \u001b[7m[[ Cell [3] Line 8 ]]\u001b[0m /tmp/gonb_0281078a/main.go:15 +0x166\n",
      "reflect.Value.call({0x4e1120?, 0x50ac10?, 0xc0000bb888?}, {0x4feef1, 0x4}, {0xc0000aaa50, 0x2, 0xc0000bba50?})\n",
      "\t/home/jupyter/go/src/reflect/value.go:596 +0xce7\n",
      "reflect.Value.Call({0x4e1120?, 0x50ac10?, 0x0?}, {0xc0000aaa50?, 0x0?, 0x0?})\n",
      "\t/home/jupyter/go/src/reflect/value.go:380 +0xb9\n",
      "github.com/gomlx/gomlx/graph.(*Exec).createAndCacheGraph(0xc0000d6080, {0xc0000be150, 0x2, 0x0?})\n",
      "\t/home/jupyter/Projects/gomlx/graph/exec.go:446 +0x777\n",
      "github.com/gomlx/gomlx/graph.(*Exec).findCacheEntry(0xc0000d6080, {0xc0000be150?, 0x2, 0x2})\n",
      "\t/home/jupyter/Projects/gomlx/graph/exec.go:500 +0x176\n",
      "github.com/gomlx/gomlx/graph.(*Exec).CallWithGraph(0xc0000d6080, {0xc0000bbf10, 0x2, 0x2})\n",
      "\t/home/jupyter/Projects/gomlx/graph/exec.go:354 +0x3ad\n",
      "github.com/gomlx/gomlx/graph.(*Exec).Call(...)\n",
      "\t/home/jupyter/Projects/gomlx/graph/exec.go:319\n",
      "main.main()\n",
      "\t \u001b[7m[[ Cell [4] Line 3 ]]\u001b[0m /tmp/gonb_0281078a/main.go:22 +0xc5\n",
      "exit status 2\n"
     ]
    }
   ],
   "source": [
    "%%\n",
    "sumExec := NewExec(manager, SumGraph)\n",
    "_ = sumExec.Call(1.1, 2) // Error: arguments have different dtypes float64 and int.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622fe6c-cbc8-46fd-8d56-eaec55e3fc9d",
   "metadata": {},
   "source": [
    "> **Note**\n",
    "> - In the stack-trace above there are 2 lines of interest, that typically help to debug such issues:\n",
    ">   1. Where in the graph building function `main.SumGraph` function the invalid operation was created: **Line 8 of the previous cell**.\n",
    ">   2. Where in the `main` function, the graph was attempted to be executed: **Line 3 of this cell**.\n",
    "> - You can enable displaying line-numbers in the JupyterLab with \"ESC+L\" (upper-case L).\n",
    "\n",
    "If you want to catch errors, **GoMLX** provides a small `exceptions` library, that defines `TryCatch[E]`, that will catch arbitrary `panic` (thrown) exceptions. **GoMLX** only throws `error` type of exceptions. So you could do:\n",
    "\n",
    "```go\n",
    "err := TryCatch[error](func() {_ = exec.Call(1.1, 2)}) // Error: different types (float64 and int) !?\n",
    "if err != nil { … }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52564b67-ae23-4545-b853-9a5c1fe384e2",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "Tensors are multidimensional arrays of a given data type (`shapes.DType`) defined in the package `gomlx/types/tensor`. \n",
    "\n",
    "For GoMLX tensors work as **containers of data** that are used as concrete inputs and outputs for the\n",
    "execution of computational graphs. There is only basic support to manipulate tensors directly (it includes access directly to its data)\n",
    "because one expects to do that with the computational graphs.  Tensors have a shape (`shapes.Shape`)\n",
    "just like `Node`.\n",
    "\n",
    "The package includes a generic `tensor.Tensor` interface, that is implemented by two types of tensors:\n",
    "`tensor.Local` and `tensor.Device`, that differ where their data is stored. \n",
    "\n",
    "The `tensor.Device` have the data in the accelerator device that is going to run the computation \n",
    "(even if it is \"Host\", which represents the CPU). These are used as input and output of the\n",
    "computation graph.\n",
    "\n",
    "`Local` means a tensor stored in local memory, and it can be directly mutated -- but generally\n",
    "we only use it to input or output data.\n",
    "\n",
    "The generic `Tensor` interface includes methods to transfer the tensor from `Local` to `Device` and vice-versa.\n",
    "It includes a cache to avoid transfering the same data multiple times. Most APIs will use `Tensor` as\n",
    "parameters and transfer as needed. \n",
    "\n",
    "There is a **cost in transferring** between `Local` and `Device`, be mindful when handling \n",
    "large data values. The API include `Finalize` to force immediate _finalization_ of a tensor, \n",
    "to free it's memory, as opposed to wait for the GC (for those very large models where GPU\n",
    "memory is at a premium).\n",
    "\n",
    "Graph execution only consumes (input) and outputs `Device` tensors, but the library provide\n",
    "all the conversion tools needed to make that simple.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6acb053e-13e3-4d79-8cc5-5cb134d0e78a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter.type=*tensor.Device, counter.shape=(Int64)[], counter=1\n",
      "counter=11\n"
     ]
    }
   ],
   "source": [
    "%%\n",
    "onePlusExec := NewExec(manager, func (x *Node) *Node {\n",
    "    return OnePlus(x) \n",
    "})\n",
    "// exec.Call will return a tensor.Device.\n",
    "counter := onePlusExec.Call(0)[0]\n",
    "// counter.Value() will first transfer counter to local with counter.Local().\n",
    "fmt.Printf(\"counter.type=%s, counter.shape=%s, counter=%v\\n\", reflect.TypeOf(counter), counter.Shape(), counter.Value())\n",
    "for ii := 0; ii < 10; ii++ {\n",
    "    // exec.Call will use counter.Device(): which uses the current value (no need to transfer) that is already\n",
    "    // on Device.\n",
    "    counter = onePlusExec.Call(counter)[0]\n",
    "}\n",
    "// counter.Value() will first convert counter to local with counter.Local().\n",
    "fmt.Printf(\"counter=%v\\n\", counter.Value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea8aeee-e88d-43c5-8a91-362099fce822",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "> - In the first call to `onePlusExec.Call(0)`, the Go constant `0` is automatically converted to a `*tensor.Device`\n",
    ">   by `Exec` and fed to the graph. It returns a `[]tensor.Tensor` with one element, containing `0+1=1`.\n",
    "> - The returned tensor is actually a `*tensor.Device` (that implements a `tensor.Tensor` interface), but the\n",
    ">   storage of the data in on device.\n",
    "> - When we loop the counter, note that we never move `counter` to a `*tensor.Local`. If we were to execute this on a \n",
    ">   GPU or TPU, the data would not have been moved back to the CPU while executing the loop -- making it faster.\n",
    "> - When we print the final result in `counter.Value()` the actual data is converted to a `tensor.Local`\n",
    ">   and its content automatically converted back to a Go `int` type.\n",
    "\n",
    "There are several ways to create `Local` tensors, the most common:\n",
    "\n",
    " - `FromValue[S](value S)`: Generics conversion, works with any supported `DType` scalar\n",
    "   as well as with any arbitrary multidimensional slice. Slices of rank > 1 must be regular, that is\n",
    "   all the sub-slices must have the same shape. E.g: `FromValue([][]float{{1,2}, {3, 5}, {7, 11}})`\n",
    " - `FromShape(shape shapes.Shape)`: creates a Local tensor with the given shape, and uninitialized values. See\n",
    "   documentation on how to mutate `Local` tensors in place with `Local.AcquireData()`.\n",
    " - `FromScalarAndDimensions[T](value T, dimensions ...int)`: creates a Local tensor with the\n",
    "   given dimensions, filled with the scalar value given. `T` must be one of the supported types.\n",
    "\n",
    "`Local` tensors provide also functions to serialize/deserialize in binary format.\n",
    "\n",
    "`Device` tensors are created only by transferring local tensors to the device, or if they are returned by the execution of a graph. \n",
    "\n",
    "See more documentation in [pkg.go](https://pkg.go.dev/github.com/gomlx/gomlx/types/tensor).\n",
    "\n",
    "Errors in the manipulation of Tensors (e.g. invalid values) are reported back with `panic`, with full stack-traces, just as\n",
    "with the `graph` package described in the previous package. The errors can easily be caught (with `recover()` or with `exceptions.TryCatch` helper) when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1593dc-b621-4307-b9fb-06376ccf1940",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradients\n",
    "\n",
    "Another important functionality required to train machine learning models based on gradient descent is calculating \n",
    "the gradients of some value being optimized with respect to some variable / quantity. \n",
    "\n",
    "GoMLX does this statically, during graph building time. It adds to the graph the computation for the gradient.\n",
    "\n",
    "Example: let's calculate the gradient of the function $f(x, y) = x^2 + xy$ for a few values of $x$ and $y$.\n",
    "Algebraically we have $df/dx(x,y) = 2x + y$ and $df/dy(x,y) = x$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1736ccc-df7c-4b8f-a6c2-312719547f69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f=[0 21 64], df/dx=[10 22 34], df/dy=[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "func f(x, y *Node) *Node {\n",
    "    return Add(Square(x), Mul(x, y))\n",
    "}\n",
    "\n",
    "%%\n",
    "gradOfFExec := NewExec(manager, func(x, y *Node) (output, gradX, gradY *Node) {\n",
    "    output = f(x, y)\n",
    "    sum := ReduceAllSum(output) // In case x and y are not scalars.\n",
    "    grads := Gradient(sum, x, y)\n",
    "    gradX, gradY = grads[0], grads[1] // df/dx, df/dy\n",
    "    return output, gradX, gradY\n",
    "})\n",
    "\n",
    "results := gradOfFExec.Call([]float64{0, 1, 2}, []float64{10, 20, 30})\n",
    "fmt.Printf(\"f=%v, df/dx=%v, df/dy=%v\\n\", results[0].Value(), results[1].Value(), results[2].Value())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afbd1b1-d53f-45be-8b95-8aab31bd3de2",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "> - For now GoMLX only calculates gradients of a scalar (typically a model loss) with respect to arbitrary tensors.\n",
    ">   It does not yet calculate **jacobians**, that is, if the value we are deriving is not a scalar. That's the reason\n",
    ">   of the `ReduceAllSum` in the example, the result is the derivative of the sum of all the 3 inputs.\n",
    "> - A question that may arise is whether it calculates the second derivative (*hessian*). In principle the machinery\n",
    ">   to do that is in place, but there are 2 limitations: (1) not all operations have their derivative implemented,\n",
    ">   in particular some of the operations that are only used when calculating the first derivative; (2) it only \n",
    ">   calculates the gradient with respect to a scalar, in most cases the hessian will be the gradient of a gradient,\n",
    ">   usually of higher rank -- Btw, contributions to the project here are welcome ;)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc04a1-30df-4a5a-aede-07dedf2a3798",
   "metadata": {},
   "source": [
    "## Variables and Context\n",
    "\n",
    "Computation graphs are [pure functions](https://en.wikipedia.org/wiki/Pure_function): they have no state,\n",
    "they take inputs, return outputs and everything in between is transient[^1].\n",
    "\n",
    "[^1]: With a few exceptions, like the random number generator.\n",
    "\n",
    "For Machine Learning as well as many types of computations, it's convenient to store intermediary results\n",
    "(the model parameters for ML) in between the execution of the computation graphs.\n",
    "\n",
    "For that GoMLX offers the `context.Context` object (completely unrelated to the usual Go's `context` package), \n",
    "and a corresponding `context.Exec`. It is a container of\n",
    "variables (whose values are tensors on device), and it manages automatically its updates, passing it as extra\n",
    "inputs and taking them out (if changed) as extra outputs of the computation graph.\n",
    "\n",
    "This may sound more complex than it is in practice. Let's see an example, where we try to find $argmin_{x}{f(x)}$ where\n",
    "$f(x) = ax^2 + bx + c$. If we solve it literally we should get, for $a > 0$, $argmin_{x}{f(x)} = \\frac{-b}{2a}$. Instead\n",
    "we solve it numerically, by gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "610daa58-952e-4d8e-8769-c2d8609baa14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import \"flag\"\n",
    "\n",
    "var (\n",
    "    flagA = flag.Float64(\"a\", 1.0, \"Value of a in the equation ax^2+bx+c\")\n",
    "    flagB = flag.Float64(\"b\", 2.0, \"Value of b in the equation ax^2+bx+c\")\n",
    "    flagC = flag.Float64(\"c\", 4.0, \"Value of c in the equation ax^2+bx+c\")\n",
    "    flagNumSteps = flag.Int(\"steps\", 10, \"Number of gradient descent steps to perform\")\n",
    "    flagLearningRate    = flag.Float64(\"lr\", 0.1, \"Initial learning rate.\")\n",
    ")\n",
    "\n",
    "// f(x) = ax^2 + bx + c\n",
    "func fGraph(x *Node) *Node {\n",
    "    f := MulScalar(Square(x), *flagA)\n",
    "    f = Add(f, MulScalar(x, *flagB))\n",
    "    f = AddScalar(f, *flagC)\n",
    "    return f\n",
    "}\n",
    "\n",
    "// minimizeF does one gradient descent on F by moving a variable \"x\",\n",
    "// and returns the value of the function at the current \"x\".\n",
    "func minimizeF(ctx *context.Context, graph *Graph) *Node {\n",
    "    xVar := ctx.VariableWithValue(\"x\", 0.0) // Variable reference.\n",
    "    x := xVar.ValueGraph(graph)             // Read variable for the current graph.\n",
    "    f := fGraph(x)                          // Value of f(x).\n",
    "    \n",
    "    // Gradient always return a slice, we take the first element for grad of X.\n",
    "    gradX := Gradient(f, x)[0] \n",
    "    \n",
    "    // stepNum += 1\n",
    "    stepNumVar := ctx.VariableWithValue(\"stepNum\", 0.0)\n",
    "    stepNum := stepNumVar.ValueGraph(graph)\n",
    "    stepNum = OnePlus(stepNum)\n",
    "    stepNumVar.SetValueGraph(stepNum)\n",
    "    \n",
    "    // step = -learningRate * gradX / Sqrt(stepNum)\n",
    "    step := Div(gradX, Sqrt(stepNum))\n",
    "    step = MulScalar(step, -*flagLearningRate)\n",
    "    \n",
    "    // x += step\n",
    "    x = Add(x, step)\n",
    "    xVar.SetValueGraph(x)\n",
    "    return f  // f(x)\n",
    "}\n",
    "\n",
    "func Solve() {\n",
    "    ctx := context.NewContext(manager)\n",
    "    exec := context.NewExec(manager, ctx, minimizeF)\n",
    "    \n",
    "    for ii := 0; ii < *flagNumSteps-1; ii++ {\n",
    "        _ = exec.Call()\n",
    "    }\n",
    "    f := exec.Call()[0]\n",
    "    x := ctx.InspectVariable(ctx.Scope(), \"x\").Value()\n",
    "    stepNum := ctx.InspectVariable(ctx.Scope(), \"stepNum\").Value()\n",
    "    fmt.Printf(\"Minimum found at x=%g, f(x)=%g after %f steps.\\n\", x.Value(), f.Value(), stepNum.Value())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d308a2-582b-49ce-b1e4-d55997b66f97",
   "metadata": {},
   "source": [
    "The code above created `Solve()` that will solve for the values set by the flags `a`, `b`, and `c`.\n",
    "\n",
    "Let's try a few values:\n",
    "\n",
    "> **Note**: `%%` in GoNB automatically creates a `func main()` and passes the extra arguments to the Go program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ba4841-21a4-4715-ac9e-6dba59b97218",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum found at x=-1, f(x)=2 after 10.000000 steps.\n"
     ]
    }
   ],
   "source": [
    "%% --a=1 --b=2 --c=3 --steps=10 --lr=0.5\n",
    "Solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff74708-1bed-49ce-b3c8-b02e9278d8fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum found at x=-3, f(x)=2 after 10.000000 steps.\n"
     ]
    }
   ],
   "source": [
    "%% --a=2 --b=12 --c=20 --steps=10 --lr=0.5\n",
    "Solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082674a7-937d-48e0-8f29-77c5bb3eee83",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "> - We are using `context.Exec`, while before we were using `computation.Exec`. The main difference is that\n",
    ">   `context.Exec` compiles and executes graph functions that take a context as its first parameter, and it \n",
    ">   automatically handles the passing of variables as side inputs and outputs (for those variables updated)\n",
    ">   of the computation graph.\n",
    "> - During graph building, we access and set the variables with `Variable.ValueGraph` and `VariableSetValueGraph`: \n",
    ">   They return/take `*Node` types, that can be used in the graph.\n",
    "> - Outside graph building, we can access the last value set to a variable by using `Variable.Value()` and \n",
    ">   `Variable.SetValue`. They return/take concrete `tensor.Tensor` types. Usually a `*tensor.Device`, the\n",
    ">   value actually being stored in the accelerator.\n",
    "> - We created two variables, one for \"x\" that we were optimizing to minimize $f(x)$, and one variable \"stepNum\",\n",
    ">   used to keep track how many steps were already executed. \n",
    "> - Yes, if we set `--lr=1` (the learning rate), it will get to the minimum in one step for the quadratic f(x). 😉\n",
    "\n",
    "There is more to `context.Context`, some we'll present on the next section on *Machine Learning*, others can\n",
    "be found in its documentation. A few things worth advancing:\n",
    "\n",
    "* `Context` is always configured at a certain *scope*, and variables are unique within its scope. Scope\n",
    "  is easily changed with `ctx.In(\"new_scope\")`. So the `Context` object is a scope (a string) and a pointer to \n",
    "  the actual data (variables, graph and model parameters).\n",
    "* `Context` also holds model parameters (concrete Go values), which are also scoped. \n",
    "  Those can be hyperparameters for the models (learning rate, regularization, etc.) or anything the user or\n",
    "  any library may create a convention for.\n",
    "* Similarly `Context` also holds \"Graph parameters\". Those are very similar to model parameters, but they\n",
    "  have one value per Graph. So if a model is created with parameters of different shape (or for training/evaluation),\n",
    "  each version will have its own Graph parameters. Don't worry about this now -- if you need it later\n",
    "  when building complex graphs, the funcitonality will be there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9edc1e-c79c-4a8e-b570-b20c0195db44",
   "metadata": {},
   "source": [
    "## Machine Learning (ML)\n",
    "\n",
    "The previous sections presented the fundamentals of what is needed to implement machine learning. This section\n",
    "we present various sub-packages that provide high level ML layers and tools that make building, training and \n",
    "evaluating a model trivial.\n",
    "\n",
    "First is the package `layers` (see code in [ml/layers](../../../tree/main/ml/layers/). It provides several composable ML layers. \n",
    "These are graph building functions, most of which take a `*context.Context` as first parameter, where they store \n",
    "variables or access hyperparameters. \n",
    "There are several such layers, for example: `layers.Dense`, \n",
    "`layers.Dropout`, `layers.PiecewiseLinearCalibration` (very good for normalization of inputs), `layers.BatchNorm`, \n",
    "`layers.LayerNorm`, `layers.Convolution`, `layers.MultiHeadAttention` (for [Transformers](https://arxiv.org/abs/1706.03762) \n",
    "layers), etc.\n",
    "\n",
    "The package `train` offers two main functionalities: `train.Trainer` will build a *train step* and an *eval step*\n",
    "graph, given a model graph building function and an optimizer. This graph can be executed in sequence to train a model.\n",
    "The package also provides `train.Loop` that simply loop over a `train.Dataset` interface, reading data and feeding\n",
    "it to `Trainer.TrainStep` or `Trainer.EvalStep`, along with executing configurable hooks on the training loop. One\n",
    "such hooks is provided by `gomlx/train/commandline.AttachProgressBar(loop)`, it pretty prints the progress during\n",
    "training on the command line.\n",
    "\n",
    "There are also a collection of optimizers, loss functions, metrics, etc. For any functionality there is always an\n",
    "example under the [examples/](../../../tree/main/examples/) subdirectory.\n",
    "\n",
    "Let's look at the simplest ML example: [`linear`](../../../tree/main/examples/linear/linear.go), \n",
    "which trains a linear model on nosiy generated data.\n",
    "\n",
    "Here are the constants of our problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d82352a6-2c89-4c00-b483-522f7c067f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "const (\n",
    "    CoefficientMu    = 0.0\n",
    "    CoefficientSigma = 5.0\n",
    "    BiasMu           = 1.0\n",
    "    BiasSigma        = 10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a176818-1ad3-416c-8de9-c52bc3a83c97",
   "metadata": {},
   "source": [
    "To generate synthetic data it first randomly chose some random coefficients and bias based on which data is \n",
    "generated. These selected coefficients is the ones we want to try to learn using ML. The coefficients could\n",
    "have been selected in Go directly using `math/random`, but just for fun, we do it using a computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83231803-576d-4504-9978-b32175ee7cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of target: coefficients=[7.09 2.38 14], bias=-4.04\n"
     ]
    }
   ],
   "source": [
    "import (\n",
    "    \"github.com/gomlx/gomlx/types/shapes\"\n",
    "    \"github.com/gomlx/gomlx/types/tensor\"\n",
    ")\n",
    "\n",
    "// initCoefficients chooses random coefficients and bias. These are the true values the model will\n",
    "// attempt to learn.\n",
    "func initCoefficients(manager *Manager, numVariables int) (coefficients, bias tensor.Tensor) {\n",
    "    e := NewExec(manager, func(g *Graph) (coefficients, bias *Node) {\n",
    "        rngState := Const(g, RngState())\n",
    "        rngState, coefficients = RandomNormal(rngState, shapes.Make(shapes.F64, numVariables))\n",
    "        coefficients = AddScalar(MulScalar(coefficients, CoefficientSigma), CoefficientMu)\n",
    "        rngState, bias = RandomNormal(rngState, shapes.Make(shapes.F64))\n",
    "        bias = AddScalar(MulScalar(bias, BiasSigma), BiasMu)\n",
    "        return\n",
    "    })\n",
    "    results := e.Call()\n",
    "    coefficients, bias = results[0], results[1]\n",
    "    return\n",
    "}\n",
    "\n",
    "%%\n",
    "coef, bias := initCoefficients(manager, 3)\n",
    "fmt.Printf(\"Example of target: coefficients=%0.3v, bias=%0.3v\\n\", coef.Value(), bias.Value()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92557ffc-7c1a-414d-9efe-f9eb0e2f4a07",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **Note**\n",
    "> - This code should look familiar, using things we presented earlier in the tutorial. It creates a computation\n",
    ">   graph to generate randomly the `coefficients` and `bias`. Then it executes it and returns the result. \n",
    "> - Notice that since the *computation graph is functional*: we need to pass around the random number\n",
    ">   generator state, which gets updated at each call to `RandomUniform` or `RandomNormal`.\n",
    ">   - Alternatively the `context.Context` introduced early can keep the state\n",
    ">     as a variable, and provides a simpler interface: see `Context.RandomUniform` and `Context.RandomNormal`.\n",
    "\n",
    "Next, we want to generate the data (examples): we generate random inputs, and then the label using the\n",
    "selected coefficients plus some normal noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee57593-a9dd-4038-a704-694a120840fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: coefficients=[-0.0768 3.32 -3.79], bias=-9.37\n",
      "5 dataset examples:\n",
      "\tx=[-0.56 1.61 -1.61]; label=[2.11]\n",
      "\tx=[0.451 -0.306 -1.81]; label=[-3.52]\n",
      "\tx=[-1.38 -0.0295 1.22]; label=[-13.6]\n",
      "\tx=[-1.33 0.103 -0.227]; label=[-8.02]\n",
      "\tx=[-0.654 -0.0458 -0.419]; label=[-7.95]\n"
     ]
    }
   ],
   "source": [
    "func buildExamples(manager *Manager, coef, bias tensor.Tensor, numExamples int, noise float64) (inputs, labels tensor.Tensor) {\n",
    "    e := NewExec(manager, func(coef, bias *Node) (inputs, labels *Node) {\n",
    "        g := coef.Graph()\n",
    "        numFeatures := coef.Shape().Dimensions[0]\n",
    "\n",
    "        // Random inputs (observations).\n",
    "        rngState := Const(g, RngState())\n",
    "        rngState, inputs = RandomNormal(rngState, shapes.Make(shapes.F64, numExamples, numFeatures))\n",
    "        coef = ExpandDims(coef, 0)\n",
    "\n",
    "        // Calculate perfect labels.\n",
    "        labels = ReduceAndKeep(Mul(inputs, coef), ReduceSum, -1)\n",
    "        labels = Add(labels, bias)\n",
    "        if noise > 0 {\n",
    "            // Add some noise to the labels.\n",
    "            var noiseVector *Node\n",
    "            rngState, noiseVector = RandomNormal(rngState, labels.Shape())\n",
    "            noiseVector = MulScalar(noiseVector, noise)\n",
    "            labels = Add(labels, noiseVector)\n",
    "        }\n",
    "        return\n",
    "    })\n",
    "    examples := e.Call(coef, bias)\n",
    "    inputs, labels = examples[0], examples[1]\n",
    "    return\n",
    "}\n",
    "\n",
    "%%\n",
    "coef, bias := initCoefficients(manager, 3)\n",
    "numExamples := 5\n",
    "inputsTensor, labelsTensor := buildExamples(manager, coef, bias, numExamples, 0.2)\n",
    "fmt.Printf(\"Target: coefficients=%0.3v, bias=%0.3v\\n\", coef.Value(), bias.Value()) \n",
    "\n",
    "fmt.Printf(\"%d dataset examples:\\n\", numExamples)\n",
    "inputs := inputsTensor.Local().Value().([][]float64)\n",
    "labels := labelsTensor.Local().Value().([][]float64)\n",
    "for ii := 0; ii < numExamples; ii ++ {\n",
    "    fmt.Printf(\"\\tx=%0.3v; label=%0.3v\\n\", inputs[ii], labels[ii])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ef923-f7cb-4011-ab84-901e0747cd68",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Now the first new concept of this section: `train.Dataset` is the interface that is used to feed data to\n",
    "during a training loop or evaluation. \n",
    "\n",
    "There are three methods: `Dataset.Yield` that returns the next batch of examples; \n",
    "`Dataset.Reset` restarts the dataset, for datasets that don't loop indefinitely; \n",
    "Finally `Dataset.Name` returns the dataset name, usually used for metric names, logging and printing.\n",
    "\n",
    "Datasets also yield a `spec`, an opaque type for GoMLX (defined as `any`), that allows the dataset to communicate\n",
    "to the model which type of data it is generating. In our case, since it's always the same data, we don't need it, \n",
    "so we keep it set to `nil`. \n",
    "If one would implement a dataset like a generic CSV file, one may want to communicate \n",
    "to the model the field names to the Model throught the `spec`, for instance. See the documentation for more details.\n",
    "\n",
    "For our linear synthetic data we implement the simplest `train.Dataset`: the whole data is pre-generated, and we return a giant batch with the full data every time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cadb18d-ef0e-4036-b450-a64f8286a65b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import \"github.com/gomlx/gomlx/ml/train\"\n",
    "\n",
    "// TrivialDataset always returns the whole data.\n",
    "type TrivialDataset struct {\n",
    "    name string\n",
    "    inputs, labels []tensor.Tensor\n",
    "}\n",
    "\n",
    "var (\n",
    "    // Assert Dataset implements train.Dataset.\n",
    "    _ train.Dataset = &TrivialDataset{}\n",
    ")\n",
    "// Name implements train.Dataset.\n",
    "func (ds *TrivialDataset) Name() string { return ds.name }\n",
    "\n",
    "// Yield implements train.Dataset.\n",
    "func (ds *TrivialDataset) Yield() (spec any, inputs, labels []tensor.Tensor, err error) {\n",
    "    return nil, ds.inputs, ds.labels, nil\n",
    "}\n",
    "\n",
    "// Reset implements train.Dataset.\n",
    "func (ds *TrivialDataset) Reset() {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1647e85-76c8-4adc-b317-2c50356231c4",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "> * More often it is more work pre-processing data than actually building an ML model ... that's life 🙁\n",
    "> * In the [examples/](../../../tree/main/examples/) subdirectory we implement `train.Dataset` for some\n",
    ">   well known data sets: UCI Adult, Cifar-10, Cifar-100, IMDB Reviews, Kaggle's Dogs vs Cats, Oxford Flowers 102.\n",
    ">   These can be used as libraries to easily try different models. \n",
    ">   If you are workig on public datasets, please consider contributing similar libraries.\n",
    "> * **GoMLX** also include the `data.InMemoryDataset`, which can be created from tensors in one line \n",
    "\n",
    "The package [github.com/gomlx/gomlx/ml/data](https://pkg.go.dev/github.com/gomlx/gomlx/ml/data) provides several\n",
    "tools to facilitate the work here:\n",
    "\n",
    "* `Parallel`: parallelizes any dataset, includes some buffer.\n",
    "* `InMemory`: reads a dataset into (accelerator) memory, and then serves it from there -- greatly accelerates training.\n",
    "  We could have used that instead of defining `TrivialDataset`, but we left it because it's common to specialize\n",
    "  datasets.\n",
    "* Downloading of datasets (with progress-bar) and checksum functions.\n",
    "\n",
    "\n",
    "### ModelFn\n",
    "\n",
    "Next we build a model, that for our `train` package means implementing a function with the following signature:\n",
    "\n",
    "```go\n",
    "type ModelFn func(ctx *context.Context, spec any, inputs []*graph.Node) (predictions []*graph.Node)\n",
    "```\n",
    "\n",
    "It takes a `context.Context` for the variables and hyperparameters, the `spec`  and a slice of `inputs` —\n",
    "the last two are fed by `Dataset.Yield` above. It returns a slice of `predictions` — is most cases there \n",
    "is just one value in the slice (only one prediction). During training `predictions` fed to the loss function,\n",
    "and during inference they can be returned directly.\n",
    "\n",
    "Our linear example has the simplest model possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40eb46ee-4850-4c5f-8422-9273529a2c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "func modelGraph(ctx *context.Context, spec any, inputs []*Node) ([]*Node) {\n",
    "    _ = spec  // Not needed here, we know the dataset.\n",
    "    logits := layers.Dense(ctx, inputs[0], /* useBias= */ true, /* outputDim= */ 1)\n",
    "    return []*Node{logits}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c60a04-0eac-4ec9-8c91-888368802f30",
   "metadata": {},
   "source": [
    "> **Note**\n",
    "> - It uses the `layers.Dense` layer, which simply multiplies the input by a learnable matrix (weights)\n",
    ">   and optionally add a learnable bias. It's the most basic building block of neural networks (NNs).\n",
    ">   The implementation of `layers.Dense` is pretty simple, and worth checking out to refresh how\n",
    ">   variables from the `Context` are used.\n",
    "> - Since it's a linear model, we don't use an activation function. The usual are available for NNs (`Relu`,\n",
    ">   `Sigmoid`, `Tanh` and more to come).\n",
    "> - The `spec` parameter allows the creation of a `ModelFn` that can be used for different types of data. The dataset\n",
    ">   can Yield also a `spec` about the type of data it is reading. Each different value of `spec` will trigger the\n",
    ">   the creation of a different computation graph, so ideally there would be at most a few types of different data\n",
    ">   source `spec`. Most commonly there is only one, like in this example, and the parameter can be ignored.\n",
    "\n",
    "### Trainer and Loop\n",
    "\n",
    "The last part is put together a `train.Trainer` and `train.Loop` objects in our `main()` function. The\n",
    "first stitches together the model, the optimizer and the loss function, and is able to run training\n",
    "steps and evaluations. The second, `train.Loop`, loops over the dataset executing a training step at\n",
    "a time and supporst a subscription (hooking) system, where one attaches things like a progress bar,\n",
    "or plotting of a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37642e67-c2c7-4ab8-a65b-9f196372efbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: coefficients=[6.16 7.99 4.48], bias=-13.8\n",
      "Training data (inputs, labels): ((Float64)[10000 3], (Float64)[10000 1])\n",
      "\n",
      "Training (100 steps):  100% [\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m] (581 steps/s)\u001b[0m [loss=0.192] [~loss=15.837]        \n",
      "\n",
      "Learned: coefficients=[[6.04] [7.81] [4.39]], bias=[-13.5]\n"
     ]
    }
   ],
   "source": [
    "import \"os\"\n",
    "import \"github.com/gomlx/gomlx/ml/train/commandline\"\n",
    "\n",
    "var (\n",
    "    flagNumExamples  = flag.Int(\"num_examples\", 10000, \"Number of examples to generate\")\n",
    "    flagNumFeatures  = flag.Int(\"num_features\", 3, \"Number of features\")\n",
    "    flagNoise        = flag.Float64(\"noise\", 0.2, \"Noise in synthetic data generation\")\n",
    "    flagNumSteps     = flag.Int(\"steps\", 100, \"Number of gradient descent steps to perform\")\n",
    ")\n",
    "\n",
    "func AssertNoError(err error) {\n",
    "    if err != nil {\n",
    "        fmt.Printf(\"Error: %+v\", err)\n",
    "        os.Exit(1)\n",
    "    }\n",
    "}\n",
    "\n",
    "// AttachToLoop is a hook to allow one to attach different functionality to the loop.\n",
    "func AttachToLoop(loop *train.Loop) {\n",
    "    commandline.AttachProgressBar(loop) // Attaches a progress bar to the loop.\n",
    "}\n",
    "\n",
    "// TrainMain() does everything to train the linear model.\n",
    "func TrainMain() {\n",
    "    flag.Parse()\n",
    "\n",
    "    // Select coefficients that we will try to predic.\n",
    "    trueCoefficients, trueBias := initCoefficients(manager, *flagNumFeatures)\n",
    "    fmt.Printf(\"Target: coefficients=%0.3v, bias=%0.3v\\n\", trueCoefficients.Value(), trueBias.Value()) \n",
    "\n",
    "    // Generate training data with noise.\n",
    "    inputs, labels := buildExamples(manager, trueCoefficients, trueBias, *flagNumExamples, *flagNoise)\n",
    "    fmt.Printf(\"Training data (inputs, labels): (%s, %s)\\n\\n\", inputs.Shape(), labels.Shape())\n",
    "    dataset := &TrivialDataset{\"linear\", []tensor.Tensor{inputs}, []tensor.Tensor{labels}}\n",
    "\n",
    "    // Creates Context with learned weights and bias.\n",
    "    ctx := context.NewContext(manager)\n",
    "    ctx.SetParam(optimizers.LearningRateKey, *flagLearningRate)\n",
    "\n",
    "    // train.Trainer executes a training step.\n",
    "    trainer := train.NewTrainer(manager, ctx, modelGraph,\n",
    "        losses.MeanSquaredError,\n",
    "        optimizers.StochasticGradientDescent(),\n",
    "        nil, nil) // trainMetrics, evalMetrics\n",
    "    loop := train.NewLoop(trainer)\n",
    "    AttachToLoop(loop)\n",
    "\n",
    "    // Loop for given number of steps.\n",
    "    _, err := loop.RunSteps(dataset, *flagNumSteps)\n",
    "    AssertNoError(err)\n",
    "\n",
    "    // Print learned coefficients and bias -- from the weights in the dense layer.\n",
    "    fmt.Println()\n",
    "    coefVar, biasVar := ctx.InspectVariable(\"/dense\", \"weights\"), ctx.InspectVariable(\"/dense\", \"biases\")\n",
    "    learnedCoef, learnedBias := coefVar.Value(), biasVar.Value()\n",
    "    fmt.Printf(\"Learned: coefficients=%0.3v, bias=%0.3v\\n\", learnedCoef.Value(), learnedBias.Value())\n",
    "}\n",
    "\n",
    "%%\n",
    "TrainMain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03efed68-cfa3-42a6-af40-e8c6d7aac87d",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "> - Hyperparameters are set on the context. Layers and optimizers can define their own\n",
    ">   hyperparemeters independently. `Context` uses a scoping system (like directories), \n",
    ">   and hyperparameters can take specialized values under specific scopes -- e.g.: \n",
    ">   doing `ctx.In(\"dense_5\").SetParam(layers.L2RegularizationKey, 0.1)` would set\n",
    ">   L2 regularization only for the layer `dense_5` of the model to `0.1`.\n",
    "> - The `trainer` constructor also takes as input arbitrary metrics (for training and evaluation).\n",
    ">   The metric of the loss of the last batch, and a moving average of the loss are always included\n",
    ">   automatically. There are many others, that can be means or moving averages, etc.\n",
    "> - The 'Loop' object is very flexible. One can attach any functionality (hooks) to be executed\n",
    ">   during training with `OnStart`, `OnStep`, `OnEnd`, `EveryNSteps`, `NTimesDuringLoop`, \n",
    ">   `PeriodicCallback` (e.g.: every N minutes) and `ExponentialCallback`.\n",
    "> - The most common such functionality is the `commandline.AttachProgressBar`. There is also a plotting\n",
    ">   of any arbitrary metric or any arbitrary `Node` in the computation graph.\n",
    "> - `Loop.RunSteps()` returns also the final metrics from the training, usually printed out.\n",
    "\n",
    "### Training and Plotting\n",
    "\n",
    "As our last example, let's train it \"for real\", that is, with more steps.\n",
    "\n",
    "And to make things prettier, let's attach also a plot of the metrics registered. In our example\n",
    "the only metrics are the default ones, the batch and mean of the loss -- the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6514ede-c84e-4259-96ee-8d6eea196815",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: coefficients=[-5.81 -0.523 -0.694], bias=-2.18\n",
      "Training data (inputs, labels): ((Float64)[10000 3], (Float64)[10000 1])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (500 steps):  100% [\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m\u001b[32m=\u001b[0m] (1963 steps/s)\u001b[0m [loss=0.040] [~loss=0.075]        \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1024\" height=\"418\" viewbox=\"0 0 1024 400\" style=\"background-color:#f8f8f8\" preserveAspectRatio=\"xMidYMid meet\"><defs><marker markerWidth=\"2%\" markerHeight=\"2%\" id=\"circle\" viewBox=\"0 0 10 10 \" refX=\"5\" refY=\"5\" markerUnits=\"userSpaceOnUse\"><circle cx=\"5\" cy=\"5\" r=\"3\" fill=\"none\" stroke=\"black\"/></marker><marker id=\"filled-circle\" viewBox=\"0 0 10 10 \" refX=\"5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"2%\" markerHeight=\"2%\"><circle stroke=\"none\" cx=\"5\" cy=\"5\" r=\"3\" fill=\"black\"/></marker><marker id=\"square\" viewBox=\"0 0 10 10 \" refX=\"5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"2%\" markerHeight=\"2%\"><rect y=\"2\" width=\"6\" height=\"6\" fill=\"none\" stroke=\"black\" x=\"2\"/></marker><marker id=\"filled-square\" viewBox=\"0 0 10 10 \" refX=\"5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"2%\" markerHeight=\"2%\"><rect x=\"2\" y=\"2\" width=\"6\" height=\"6\" fill=\"black\" stroke=\"none\"/></marker></defs><g marker-mid=\"url(#square)\" stroke-linecap=\"round\" stroke-linejoin=\"round\" marker-end=\"url(#square)\" stroke=\"hsl(90, 47%, 65%)\" fill=\"none\" stroke-width=\"2px\" transform=\"translate(70 330 )scale(1 -1 )\" marker-start=\"url(#square)\"><path d=\"M1.768000e+01,2.548000e+02 L3.298613e+01,9.690883e+01 L4.999295e+01,6.007225e+01 L6.699976e+01,4.404288e+01 L8.400657e+01,3.509325e+01 L1.010134e+02,2.940967e+01 L1.180202e+02,2.549567e+01 L1.350270e+02,2.264419e+01 L1.520338e+02,2.047854e+01 L1.690406e+02,1.878017e+01 L1.860475e+02,1.741388e+01 L2.030543e+02,1.623356e+01 L2.200611e+02,1.516321e+01 L2.370679e+02,1.419331e+01 L2.540747e+02,1.331490e+01 L2.710815e+02,1.251963e+01 L2.880883e+02,1.179981e+01 L3.050952e+02,1.114841e+01 L3.221020e+02,1.055900e+01 L3.391088e+02,1.002574e+01 L3.561156e+02,9.543324e+00 L3.731224e+02,9.106924e+00 L3.901292e+02,8.712172e+00 L4.071360e+02,8.355105e+00 L4.241428e+02,8.032136e+00 L4.411497e+02,7.740015e+00 L4.581565e+02,7.475801e+00 L4.751633e+02,7.236832e+00 L4.921701e+02,7.020697e+00 L5.091769e+02,6.825217e+00 L5.261837e+02,6.648421e+00 L5.431905e+02,6.488522e+00 L5.601974e+02,6.343907e+00 L5.772042e+02,6.213115e+00 L5.942110e+02,6.094827e+00 L6.112178e+02,5.987846e+00 L6.282246e+02,5.891093e+00 L6.452314e+02,5.803589e+00 L6.622382e+02,5.724451e+00 L6.792451e+02,5.652879e+00 L6.962519e+02,5.588150e+00 L7.132587e+02,5.529609e+00 L7.302655e+02,5.476665e+00 L7.472723e+02,5.428783e+00 L7.642791e+02,5.385479e+00 L7.812859e+02,5.346316e+00 L7.982927e+02,5.310897e+00 L8.152996e+02,5.278864e+00 L8.323064e+02,5.249895e+00 L8.493132e+02,5.223695e+00 L8.663200e+02,5.200000e+00 \" vector-effect=\"non-scaling-stroke\"/></g><g stroke=\"black\" transform=\"translate(70 330 )scale(1 -1 )\" fill=\"none\" stroke-width=\"2px\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path vector-effect=\"non-scaling-stroke\" d=\"M1.768000e+01,0 L1.768000e+01,-6 M6.870044e+01,0 L6.870044e+01,-6 M1.197209e+02,0 L1.197209e+02,-6 M1.707413e+02,0 L1.707413e+02,-6 M2.217618e+02,0 L2.217618e+02,-6 M2.727822e+02,0 L2.727822e+02,-6 M3.238026e+02,0 L3.238026e+02,-6 M3.748231e+02,0 L3.748231e+02,-6 M4.258435e+02,0 L4.258435e+02,-6 M4.768640e+02,0 L4.768640e+02,-6 M5.278844e+02,0 L5.278844e+02,-6 M5.789048e+02,0 L5.789048e+02,-6 M6.299253e+02,0 L6.299253e+02,-6 M6.809457e+02,0 L6.809457e+02,-6 M7.319662e+02,0 L7.319662e+02,-6 M7.829866e+02,0 L7.829866e+02,-6 M8.340071e+02,0 L8.340071e+02,-6 \"/></g><g text-anchor=\"middle\" font-weight=\"normal\" font-size=\"12px\" fill=\"black\" transform=\"translate(70 330 )scale(1 1 )\" font-family=\"sans-serif\" font-style=\"normal\" stroke-width=\"2px\" dominant-baseline=\"hanging\" stroke=\"black\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><text stroke=\"none\" vector-effect=\"non-scaling-stroke\" x=\"1.768000e+01\" y=\"10\" dominant-baseline=\"hanging\">0</text><text x=\"6.870044e+01\" y=\"10\" dominant-baseline=\"hanging\" stroke=\"none\" vector-effect=\"non-scaling-stroke\">30</text><text stroke=\"none\" vector-effect=\"non-scaling-stroke\" x=\"1.197209e+02\" y=\"10\" dominant-baseline=\"hanging\">60</text><text vector-effect=\"non-scaling-stroke\" x=\"1.707413e+02\" y=\"10\" dominant-baseline=\"hanging\" stroke=\"none\">90</text><text vector-effect=\"non-scaling-stroke\" x=\"2.217618e+02\" y=\"10\" dominant-baseline=\"hanging\" stroke=\"none\">120</text><text x=\"2.727822e+02\" y=\"10\" dominant-baseline=\"hanging\" stroke=\"none\" vector-effect=\"non-scaling-stroke\">150</text><text stroke=\"none\" vector-effect=\"non-scaling-stroke\" x=\"3.238026e+02\" y=\"10\" dominant-baseline=\"hanging\">180</text><text stroke=\"none\" vector-effect=\"non-scaling-stroke\" x=\"3.748231e+02\" y=\"10\" dominant-baseline=\"hanging\">210</text><text y=\"10\" dominant-baseline=\"hanging\" stroke=\"none\" vector-effect=\"non-scaling-stroke\" x=\"4.258435e+02\">240</text><text y=\"10\" dominant-baseline=\"hanging\" stroke=\"none\" vector-effect=\"non-scaling-stroke\" x=\"4.768640e+02\">270</text><text x=\"5.278844e+02\" y=\"10\" dominant-baseline=\"hanging\" stroke=\"none\" vector-effect=\"non-scaling-stroke\">300</text><text y=\"10\" dominant-baseline=\"hanging\" stroke=\"none\" vector-effect=\"non-scaling-stroke\" x=\"5.789048e+02\">330</text><text y=\"10\" dominant-baseline=\"hanging\" stroke=\"none\" vector-effect=\"non-scaling-stroke\" x=\"6.299253e+02\">360</text><text x=\"6.809457e+02\" y=\"10\" dominant-baseline=\"hanging\" stroke=\"none\" vector-effect=\"non-scaling-stroke\">390</text><text stroke=\"none\" vector-effect=\"non-scaling-stroke\" x=\"7.319662e+02\" y=\"10\" dominant-baseline=\"hanging\">420</text><text vector-effect=\"non-scaling-stroke\" x=\"7.829866e+02\" y=\"10\" dominant-baseline=\"hanging\" stroke=\"none\">450</text><text stroke=\"none\" vector-effect=\"non-scaling-stroke\" x=\"8.340071e+02\" y=\"10\" dominant-baseline=\"hanging\">480</text></g><g font-style=\"normal\" dominant-baseline=\"baseline\" fill=\"black\" stroke-linejoin=\"round\" font-family=\"sans-serif\" font-size=\"12px\" font-weight=\"bold\" text-anchor=\"middle\" stroke-width=\"2px\" stroke-linecap=\"round\" transform=\"translate(70 330 )scale(1 1 )rotate(0 0 0 )\" stroke=\"black\"><text y=\"-6\" dominant-baseline=\"baseline\" stroke=\"none\" vector-effect=\"non-scaling-stroke\" x=\"442\">Steps</text></g><g font-size=\"12px\" font-style=\"normal\" stroke=\"black\" stroke-linejoin=\"round\" font-family=\"sans-serif\" fill=\"black\" stroke-width=\"2px\" stroke-linecap=\"round\" transform=\"translate(70 330 )scale(1 -1 )\" font-weight=\"bold\" text-anchor=\"middle\" dominant-baseline=\"baseline\"><path vector-effect=\"non-scaling-stroke\" d=\"M0,6.860564e+01 L-6,6.860564e+01 M0,1.324910e+02 L-6,1.324910e+02 M0,1.963764e+02 L-6,1.963764e+02 \"/></g><g stroke-width=\"2px\" font-family=\"sans-serif\" stroke=\"black\" text-anchor=\"end\" font-size=\"12px\" font-style=\"normal\" font-weight=\"normal\" dominant-baseline=\"middle\" fill=\"black\" stroke-linecap=\"round\" stroke-linejoin=\"round\" transform=\"translate(70 330 )scale(1 1 )\"><text vector-effect=\"non-scaling-stroke\" x=\"-10\" y=\"-6.860564e+01\" dominant-baseline=\"middle\" stroke=\"none\">10.000</text><text x=\"-10\" y=\"-1.324910e+02\" dominant-baseline=\"middle\" stroke=\"none\" vector-effect=\"non-scaling-stroke\">20.000</text><text x=\"-10\" y=\"-1.963764e+02\" dominant-baseline=\"middle\" stroke=\"none\" vector-effect=\"non-scaling-stroke\">30.000</text></g><g fill=\"black\" stroke-width=\"2px\" stroke-linecap=\"round\" font-size=\"12px\" font-style=\"normal\" stroke=\"black\" text-anchor=\"middle\" dominant-baseline=\"hanging\" stroke-linejoin=\"round\" transform=\"translate(70 330 )scale(1 1 )rotate(-90 0 0 )\" font-family=\"sans-serif\" font-weight=\"bold\"><text stroke=\"none\" vector-effect=\"non-scaling-stroke\" x=\"130\" y=\"6\" dominant-baseline=\"hanging\">loss</text></g><g transform=\"translate(70 330 )scale(1 -1 )\" font-family=\"sans-serif\" font-size=\"12px\" font-style=\"normal\" fill=\"black\" stroke=\"gray\" font-weight=\"bold\" dominant-baseline=\"hanging\" stroke-width=\"0.5px\" stroke-linecap=\"round\" stroke-linejoin=\"round\" text-anchor=\"middle\"><path vector-effect=\"non-scaling-stroke\" d=\"M0,6.860564e+01 L884,6.860564e+01 M0,1.324910e+02 L884,1.324910e+02 M0,1.963764e+02 L884,1.963764e+02 \"/></g><g dominant-baseline=\"hanging\" stroke=\"black\" text-anchor=\"middle\" font-family=\"sans-serif\" font-style=\"normal\" stroke-linecap=\"round\" font-weight=\"bold\" stroke-linejoin=\"round\" font-size=\"12px\" fill=\"none\" stroke-width=\"2px\"><rect y=\"70\" width=\"884\" height=\"260\" vector-effect=\"non-scaling-stroke\" x=\"70\"/><g font-size=\"18px\" fill=\"black\" dominant-baseline=\"middle\"><text x=\"512\" y=\"35\" dominant-baseline=\"middle\" stroke=\"none\" vector-effect=\"non-scaling-stroke\">loss metrics</text></g><g stroke=\"hsl(90, 47%, 65%)\" font-weight=\"normal\" text-anchor=\"start\" dominant-baseline=\"hanging\" fill=\"hsl(90, 47%, 65%)\" stroke-linejoin=\"round\" font-family=\"sans-serif\" font-style=\"normal\" stroke-width=\"1px\" stroke-linecap=\"round\" font-size=\"12px\"><rect width=\"12\" height=\"12\" vector-effect=\"non-scaling-stroke\" x=\"76\" y=\"366\"/><g fill=\"black\" stroke=\"black\"><text y=\"366\" dominant-baseline=\"hanging\" stroke=\"none\" vector-effect=\"non-scaling-stroke\" x=\"92\">Train: Moving Average Loss</text></g></g></g></svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned: coefficients=[[-5.8] [-0.524] [-0.696]], bias=[-2.18]\n"
     ]
    }
   ],
   "source": [
    "import (\n",
    "    \"github.com/gomlx/gomlx/examples/notebook/gonb/margaid\"\n",
    "    \"github.com/gomlx/gomlx/ml/train\"\n",
    "    \"github.com/gomlx/gomlx/ml/train/commandline\"\n",
    ")\n",
    "\n",
    "func AttachToLoop(loop *train.Loop) {\n",
    "    commandline.AttachProgressBar(loop) // Attaches a progress bar to the loop. \n",
    "    margaid.New(1024, 400).DynamicUpdates().Attach(loop, /* num plot points: */ 50)  // Generates a new plot point at every step.\n",
    "}\n",
    "\n",
    "%% --steps=500\n",
    "TrainMain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268a8d3-148d-4db5-9289-573d36b73412",
   "metadata": {},
   "source": [
    "> **Note**:\n",
    "> The `gomlx/examples/notebook/gonb/margaid` package will automatically plot all the metrics registered in the trainer. When it attaches itself to \n",
    "> `loop` it collects the metric values (and run evaluation on any requested datasets), and at the end of it, plot it. Optionally, with `Plots.DynamicUpdates()`\n",
    "> it also plots intermediary results, displaying the progress of the training as it happens -- it's too fast here to notice.\n",
    "\n",
    "### Other examples\n",
    "\n",
    "There is much more in the libraries and examples. \n",
    "Mostly, it's well documented, and the implementation is generally simple to use and understand. \n",
    "For learning ML, we highly recommend looking at the `layers` library, to learn how many of the common ML layers work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71408a0-1763-4ebe-b7f7-912de9866863",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Debugging\n",
    "\n",
    "Unfortunately, the computers just \"don't get it\": they do exactly what we tell them to do,\n",
    "as opposed to what we want them to do, and thus programs fail or crash. \n",
    "GoMLX provides different ways to track down various types of errors. The most commonly used below:\n",
    "\n",
    "### Good old \"printf\"\n",
    "\n",
    "It's convenient because of Go fast compilation (change something and run to see what one gets is\n",
    "almost instant). Logging results to stdout is a valid way of developing. During graph building development,\n",
    "often one prints the shape of the `Node` being operated to confirm (or not) one's expectations.\n",
    "\n",
    "### Errors reported with `panic` -- Go \"exceptions\".\n",
    "\n",
    "Errors during the building of the graph are reported back with `panic`. These can be caught in Go with `recover`,\n",
    "but **GoMLX** offers the `exceptions` library to make it easy. \n",
    "All libraries always `panic` helpful error messages -- they can be printed with `\"%+v\"` to get \n",
    "full stack-trace output.\n",
    "\n",
    "### Node Shape Asserts\n",
    "\n",
    "During the writing of complex models, it's very common to add comments on the expected shapes of the graph nodes, to\n",
    "facilitate the reader (and developer) of the code to have the right mental model of what is going on.\n",
    "\n",
    "GoMLX provides a series of _assert_ methods that can be used instead. They serve both as documentation, and an early\n",
    "exit in case of some unexpected results. \n",
    "\n",
    "For example, a `modelGraph` function could contain:\n",
    "\n",
    "```go\n",
    "    batch_size := inputs[0].Shape().Dimensions[0]\n",
    "    ...\n",
    "    layer := Concatenate(allEmbeddings, -1)\n",
    "    layer.AssertDims(batchSize, -1)  // 2D tensor, with batch size as the leading dimension.\n",
    "```\n",
    "\n",
    "Although using these when building graphs is the most common case, there are similar assert functions for tensors and shapes themselves in the package `gomlx/types/shapes`.\n",
    "\n",
    "\n",
    "### Graph Execution Logging\n",
    "\n",
    "Every `Node` of the graph can be flagged with `SetLogged(msg)`.\n",
    "The executor (`Exec`) will at the end of the execution log all these values. The default logger \n",
    "(set with `Exec.SetLogger`) will simply print the message `msg` along with the value of the `Node` of\n",
    "interest. Creating a specialized loggers that handle arbitrary nodes is trivial.\n",
    "\n",
    "### Catching `NaN` and `Inf` in your training\n",
    "\n",
    "This is a common source of headaches when training complex models. \n",
    "The package `nanlogger` implements a specialized logger and monitor results on arbitrary nodes in the graph,\n",
    "and will `panic` with custom messages (and a stack-trace) a the first sight of a `NaN` (or `±Inf`).\n",
    "\n",
    "### More Debugging\n",
    "\n",
    "Tests and these methods have been enough to develop most of GoMLX so far. But there are other\n",
    "debugging tools that could be made, see discussion in the [Debugging](https://github.com/gomlx/gomlx/blob/main/docs/debugging.md) document. Let us know if you need something specialized.\n",
    "\n",
    "---\n",
    "\n",
    "Happy coding and [good luck](https://arxiv.org/abs/1803.03635) on modeling!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb81912-1bcb-4e28-a397-52e96b20744b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img alt=\"[Zürich See]\" src=\"zurich_see.jpg\" style=\"width:100%;\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Go (gonb)",
   "language": "go",
   "name": "gonb"
  },
  "language_info": {
   "codemirror_mode": "",
   "file_extension": ".go",
   "mimetype": "",
   "name": "go",
   "nbconvert_exporter": "",
   "pygments_lexer": "",
   "version": "go1.21.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
