/***** File generated by ./cmd/backends_codegen, based on github.com/gomlx/gopjrt. Don't edit it directly. *****/

package backends

import (
	"github.com/gomlx/gomlx/types/shapes"
	"github.com/gomlx/gopjrt/dtypes"
	"github.com/gomlx/gopjrt/protos"
)

type StandardOps interface {
	// Abs returns the Op that represents the output of the corresponding operation.
	Abs(x Op) Op

	// Add returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	Add(x0, x1 Op) Op

	// And returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	And(x0, x1 Op) Op

	// ArgMinMax calculates the "argmin" or "argmax" across an axis of the given input array x.
	// outputDType defines the output of the argmin/argmax, it doesn't need to be the same as the input.
	// It's a form of reduction on the given axis, and that axis goes away. So the rank of the result is one less than
	// the rank of x.
	// Examples:
	// 	ArgMinMax(x={{2, 0, 7}, {-3, 4, 2}}, axis=1, isMin=true) -> {1, 0}  // (it chooses the 0 and the -3)
	// 	ArgMinMax(x={{2, 0, 7}, {-3, 4, 2}}, axis=0, isMin=false) -> {0, 1, 0} // (it choose the 2, 4 and 7)
	ArgMinMax(x Op, axis int, outputDType dtypes.DType, isMin bool) Op

	// BatchNormInference implements Batch Norm for inference. See details in
	// https://www.tensorflow.org/xla/operation_semantics#batchnorminference.
	// Based on paper "Batch Normalization: Accelerating Deep Network Training by Reducing
	// Internal Covariate Shift" (Sergey Ioffe, Christian Szegedy), https://arxiv.org/abs/1502.03167.
	BatchNormInference(operand, scale, offset, mean, variance Op, epsilon float32, axis int) Op

	// Broadcast prefixes dimensions to an array by duplicating the data in the array.
	// See BroadcastInDim for a broadcast in between the axes.
	// The new dimensions dims are inserted on the left, i.e., if
	// prefixDims has values `{a0, ..., aN}` and the operand shape
	// has dimensions {b0, ..., bM} then the shape of the output has
	// dimensions {a0, ..., aN, b0, ..., bM}.
	// The new dimensions id into copies of the operand, i.e.
	// 	output[i0, ..., iN, j0, ..., jM] = operand[j0, ..., jM]
	Broadcast(x Op, prefixDims ...int) Op

	// BroadcastInDim broadcasts x to an output with the given shape.
	// broadcastAxes has an output axes value for each x axes (len(broadcastAxes) == x.Shape.Rank()).
	// The i-th axis of x is mapped to the broadcastDim[i]-th dimension of the output.
	// broadcastAxes must be also increasing: this operation cannot be used to transpose axes, it will only
	// broadcast and introduce new axes in-between.
	// This also requires that the i-th input dimension is either 1 or is the same as the
	// output dimension it's broadcasting into.
	// For example, say operand `x = (s32)[2]{1, 2}`; outputShape = `(s32)[2,2]`:
	//   - Specifying []int{1} as broadcast_dimension will generate output
	//     {{1, 2},
	//     {1, 2}}
	//   - On the other hand, specifying []int{0} as broadcast_dimension
	//     will generate output
	//     {{1 , 1},
	//     {2 , 2}}
	BroadcastInDim(x Op, outputShape shapes.Shape, broadcastAxes []int) Op

	// Ceil returns the Op that represents the output of the corresponding operation.
	Ceil(x Op) Op

	// Clz returns the Op that represents the output of the corresponding operation.
	Clz(x Op) Op

	// Complex returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	Complex(x0, x1 Op) Op

	// Concatenate results on the given axis.
	// All axes that are not being concatenated must match dimensions.
	// It doesn't work with scalars -- use ExpandDims.
	// If there is only one operand, it is returned and this is a no-op.
	Concatenate(axis int, operands ...Op) Op

	// Conj returns the Op that represents the output of the corresponding operation.
	Conj(x Op) Op

	// ConvGeneralDilated is a generic Convolution operation offered by XLA.
	// featureAxisAfter defines whether the features (aka. channels or depth) axis comes after the
	// spatial dimension. Example: a 2D input can be one of the two:
	//   - featureAxisAfter=false: input=[batch_size, features, height, width], filter=[output_features, input_features, height, width]
	//   - featureAxisAfter=true:  input=[batch_size, height, width, features], filter=[output_features, height, width, input_features]
	// Some details in https://www.tensorflow.org/xla/operation_semantics#convwithgeneralpadding_convolution.
	// There operand and filter are called lhs and rhs.
	// (XLA documentation is unfortunately poor, much is guess-work).
	// Also useful, https://arxiv.org/pdf/1603.07285v1.pdf.
	ConvGeneralDilated(operand, filter Op, axes ConvolveAxesConfig, strides []int, paddings [][2]int, inputDilation, filterDilation []int, filterGroupCount, batchGroupCount int) Op

	// ConvertDType of x to dtype.
	ConvertDType(x Op, dtype dtypes.DType) Op

	// Cos returns the Op that represents the output of the corresponding operation.
	Cos(x Op) Op

	// Div returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	Div(x0, x1 Op) Op

	// Dot returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	Dot(x0, x1 Op) Op

	// DotGeneral takes as input lhs (left-hand-side) and rhs (right-hand-side) specifications
	// for a general vector product -- a generalized "Einsum". Each axis can be:
	//   - Just aligned (batch axes), so the output has the same axes as the inputs. The dimensions
	//     must match in lhs and rhs.
	//   - Crossed (default), in which case the output is the combination (concatenation) of the
	//     dimensions.
	//   - Contracted (contracting axes), where the output does multiply the values and reduce sum
	//     those dimensions.
	// It follows that the resulting dimension number starts with the batch dimension, then the 'lhs'
	// non-contracting/non-batch dimension, and finally the 'rhs' non-contracting/non-batch dimension.
	// It provides the basic means of implementing Einsum.
	DotGeneral(lhs Op, lhsContractingAxes, lhsBatchAxes []int, rhs Op, rhsContractingAxes, rhsBatchAxes []int) Op

	// Equal returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	Equal(x0, x1 Op) Op

	// EqualTotalOrder returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	EqualTotalOrder(x0, x1 Op) Op

	// Exp returns the Op that represents the output of the corresponding operation.
	Exp(x Op) Op

	// Expm1 returns the Op that represents the output of the corresponding operation.
	Expm1(x Op) Op

	// FFT calls the XLA FFT operation, which implements {Forward, Inverse} x {Complex, Real} versions.
	// See documentation in https://www.tensorflow.org/xla/operation_semantics.
	// Underlying, CPU FFT is backed by Eigen's TensorFFT and GPU FFT uses cuFFT.
	FFT(operand Op, fftType protos.FftType, fftLength []int) Op

	// Floor returns the Op that represents the output of the corresponding operation.
	Floor(x Op) Op

	// Gather is a powerful but cumbersome Gather operation offered by XLA.
	// Full details in https://www.tensorflow.org/xla/operation_semantics#gather.
	// (Warning: it's poorly described, with many undefined terms)
	// Arguments:
	//   - startIndices: are the indices we want to gather. There will be one axis with which enumerates the indices
	//     in the operand array, typically the last one. All other axes are "batch dimensions" and they will have
	//     equivalent axes in the output.
	//   - indexVectorAxis: typically the last axis of startIndices, so startIndices.Shape.Rank()-1.
	//     Usually, one has the dimension of the indexVectorAxis equal to the full rank of the operand.
	//     That is: startIndices.Shape.Dimensions[indexVectorAxis] = operand.Shape.Rank()
	//     Lets call "one index vector" a value of startIndices formed by a slice across indexVectorAxis.
	//   - startIndexMap: for each "index vector" from startIndices, this maps each element of the vector goes to
	//     which axes of the operand. Typically, this is [0, 1, 2, ..., operand.Shape.Rank()-1], that is, each
	//     "index vector" fully defines an element on the operand. If one is gathering slices of the operand (as
	//     opposed to individual values), one can skip some of those axes from startIndexMap, and the index for those
	//     axis is considered 0, and set sliceSizes to take the slice one wants (typically the full slice).
	//   - sliceSizes: the "index vector" described above points to the data in the operand to be gathered. Then sliceSizes
	//     indicates how much data to gather. One value per axis of the operand must be set. For gathering individual
	//     values, set these all to 1.
	//   - collapsedSliceAxes: the slice gathered for each "index vector" (with sizes sliceSizes), often has dimension one
	//     for most (or all, in case of gathering individual items) axes. collapsedSliceAxes allows one to collapse those
	//     axes, so they don't show up in the output. Usually, collapse all axes that are size one.
	//     These are axes within the rank of operand (from 0 to operand.Shape.Rank()-1).
	//   - offsetAxes: for those gathered slices not collapsed (with collapsedSliceAxes), this maps them to a position in
	//     the output array. Typically, these will be consecutive numbers starting with indexVectorAxis. So, the output
	//     will have the same prefix shape (the "batch dimensions") as the startIndices array, and the suffix shape will
	//     be the gathered slices mapped to these `offsetAxes`. There must be one value per axis not collapsed with
	//     collapsedSliceAxes -- the value itself is an axis in the output shape.
	Gather(operand, startIndices Op, indexVectorAxis int, offsetAxes, collapsedSliceAxes, startIndexMap, sliceSizes []int, indicesAreSorted bool) Op

	// GreaterOrEqual returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	GreaterOrEqual(x0, x1 Op) Op

	// GreaterOrEqualTotalOrder returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	GreaterOrEqualTotalOrder(x0, x1 Op) Op

	// GreaterThan returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	GreaterThan(x0, x1 Op) Op

	// GreaterThanTotalOrder returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	GreaterThanTotalOrder(x0, x1 Op) Op

	// Imag returns the Op that represents the output of the corresponding operation.
	Imag(x Op) Op

	// Iota creates a constant of the given shape with increasing numbers (starting from 0)
	// on the given axis. So Iota([2,2], 1) returns [[0 1][0 1]], while Iota([2,2], 0)
	// returns [[0 0][1 1]].
	Iota(shape shapes.Shape, iotaAxis int) Op

	// LessOrEqual returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	LessOrEqual(x0, x1 Op) Op

	// LessOrEqualTotalOrder returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	LessOrEqualTotalOrder(x0, x1 Op) Op

	// LessThan returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	LessThan(x0, x1 Op) Op

	// LessThanTotalOrder returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	LessThanTotalOrder(x0, x1 Op) Op

	// Log returns the Op that represents the output of the corresponding operation.
	Log(x Op) Op

	// Log1p returns the Op that represents the output of the corresponding operation.
	Log1p(x Op) Op

	// LogicalNot returns the Op that represents the output of the corresponding operation.
	LogicalNot(x Op) Op

	// Logistic returns the Op that represents the output of the corresponding operation.
	Logistic(x Op) Op

	// Max returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	Max(x0, x1 Op) Op

	// Min returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	Min(x0, x1 Op) Op

	// Mul returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	Mul(x0, x1 Op) Op

	// Neg returns the Op that represents the output of the corresponding operation.
	Neg(x Op) Op

	// NotEqual returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	NotEqual(x0, x1 Op) Op

	// NotEqualTotalOrder returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	NotEqualTotalOrder(x0, x1 Op) Op

	// Or returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	Or(x0, x1 Op) Op

	// Pad injects padding on the start, end or interior (in between each element) of the given operand.
	// There must be at most `operand.Rank()` axesConfig values. Missing PadAxis are assumed to be zeros,
	// that is, no padding for those axes.
	Pad(x, fillValue Op, axesConfig ...PadAxis) Op

	// Pow returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	Pow(x0, x1 Op) Op

	// Real returns the Op that represents the output of the corresponding operation.
	Real(x Op) Op

	// ReduceMax is a shortcut for Reduce with the proper computation and initial value to reduce x on the given axes, by taking the max value.
	// If no axes are given, it reduces the full array.
	ReduceMax(x Op, axes ...int) Op

	// ReduceMin is a shortcut for Reduce with the proper computation and initial value to reduce x on the given axes, by taking the min value.
	// If no axes are given, it reduces the full array.
	ReduceMin(x Op, axes ...int) Op

	// ReduceProduct is a shortcut for Reduce with the proper computation and initial value to reduce x on the given axes, by taking the product of the reduced axes.
	// If no axes are given, it reduces the full array.
	ReduceProduct(x Op, axes ...int) Op

	// ReduceSum is a shortcut for Reduce with the proper computation and initial value to reduce x on the given axes, by taking the sum of the reduced axes.
	// If no axes are given, it reduces the full array.
	ReduceSum(x Op, axes ...int) Op

	// Rem returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	Rem(x0, x1 Op) Op

	// Reshape reshapes x to the new dimensions.
	// Total size cannot change, it's just a "reinterpretation" of the same flat data.
	// The dtype remains the same, see ConvertDType to actually convert the values.
	Reshape(x Op, dimensions ...int) Op

	// Reverse returns x with the values for the given dimensions reversed, that is,
	// the value indexed at `i` will be swapped with the value at indexed `(dimension_size - 1 - i)`.
	// The shape remains the same.
	Reverse(x Op, axes ...int) Op

	// Round returns the Op that represents the output of the corresponding operation.
	Round(x Op) Op

	// Rsqrt returns the Op that represents the output of the corresponding operation.
	Rsqrt(x Op) Op

	// ScalarOne returns a one (1) constant for the given dtype.
	// It caches the constant, so it doesn't get defined multiple times.
	ScalarOne(dtype dtypes.DType) Op

	// ScalarZero returns a zero constant for the given dtype.
	// It caches the constant, so it doesn't get defined multiple times.
	ScalarZero(dtype dtypes.DType) Op

	// ScatterAdd values from updates pointed by scatterIndices to operand.
	ScatterAdd(operand, scatterIndices, updates Op, indexVectorAxis int, updateWindowAxes, insertedWindowAxes, scatterAxesToOperandAxes []int, indicesAreSorted, uniqueIndices bool) Op

	// ScatterMax scatter values from updates pointed by scatterIndices to operand, by taking the Max.
	ScatterMax(operand, scatterIndices, updates Op, indexVectorAxis int, updateWindowAxes, insertedWindowAxes, scatterAxesToOperandAxes []int, indicesAreSorted, uniqueIndices bool) Op

	// ScatterMin scatter values from updates pointed by scatterIndices to operand, by taking the Min.
	ScatterMin(operand, scatterIndices, updates Op, indexVectorAxis int, updateWindowAxes, insertedWindowAxes, scatterAxesToOperandAxes []int, indicesAreSorted, uniqueIndices bool) Op

	// SelectAndScatterMax runs windows (similar to ReduceWindow) over the operand, selects values to updates the output (like ScatterAdd)
	// It selects the values in the window such that it works as reverse for ScatterMax.
	// See details in https://openxla.org/xla/operation_semantics#selectandscatter
	SelectAndScatterMax(operand, source Op, windowDimensions, windowStrides []int, paddings [][2]int) Op

	// SelectAndScatterMin runs windows (similar to ReduceWindow) over the operand, selects values to updates the output (like ScatterAdd)
	// It selects the values in the window such that it works as reverse for ScatterMin.
	// See details in https://openxla.org/xla/operation_semantics#selectandscatter
	SelectAndScatterMin(operand, source Op, windowDimensions, windowStrides []int, paddings [][2]int) Op

	// SelectAndScatterSum runs windows (similar to ReduceWindow) over the operand, selects values to updates the output (like ScatterAdd)
	// It selects the values in the window such that it works as reverse for ScatterSum.
	// See details in https://openxla.org/xla/operation_semantics#selectandscatter
	SelectAndScatterSum(operand, source Op, windowDimensions, windowStrides []int, paddings [][2]int) Op

	// Sign returns the Op that represents the output of the corresponding operation.
	Sign(x Op) Op

	// Sin returns the Op that represents the output of the corresponding operation.
	Sin(x Op) Op

	// Slice extracts a sub-array from the input array.
	// The sub-array is of the same rank as the input and contains the values inside a bounding box within the input array
	// where the dimensions and indices of the bounding box are given as arguments to the slice operation.
	// The strides set the input stride of the slice in each axis and must be >= 1.
	// It is optional, and if missing it is assumed to be 1 for every dimension.
	// Examples:
	// 	Slice(x={0, 1, 2, 3, 4}, starts={2}, limits={4}, strides=nil) -> {2, 3}
	// 	Slice(x={0, 1, 2, 3, 4}, starts={2}, limits={5}, strides={2}) -> {2, 4}
	Slice(x Op, starts, limits, strides []int) Op

	// Sqrt returns the Op that represents the output of the corresponding operation.
	Sqrt(x Op) Op

	// Sub returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	Sub(x0, x1 Op) Op

	// Tanh returns the Op that represents the output of the corresponding operation.
	Tanh(x Op) Op

	// Transpose axes of x.
	// There should be one value in permutations for each axis in x.
	// The output will have: output.Shape.Dimension[permutation[i]] = x.Shape.Dimension[i].
	Transpose(x Op, permutations ...int) Op

	// Where takes element-wise values from onTrue or onFalse depending on the value of condition (expected to be boolean).
	Where(condition, onTrue, onFalse Op) Op

	// Xor returns the Op that represents the output of the corresponding operation.
	// The op is created on the same XlaBuilder as used for x0 and x1.
	Xor(x0, x1 Op) Op
}
