// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build arm64

package packgemm

import (
	"github.com/ajroetker/go-highway/hwy"
	"github.com/ajroetker/go-highway/hwy/asm"
	"unsafe"
)

func BasePackRHS_neon_Float16(src []hwy.Float16, dst []hwy.Float16, srcRowStart int, srcColStart int, srcRowStride int, contractingRows int, numCols int, kernelCols int) {
	dstIdx := 0
	numFullStrips := numCols / kernelCols
	fullStripsCol := numFullStrips * kernelCols
	srcStartRowIdx := srcRowStart * srcRowStride
	useScalar := true
	if hwy.CurrentLevel() != hwy.DispatchScalar {
		numLanes := 8
		switch {
		case kernelCols == numLanes:
			useScalar = false
			var v0 hwy.Vec[hwy.Float16]
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = hwy.Load(src[srcIdx:])
					hwy.StoreFull(v0, dst[dstIdx:])
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*2:
			useScalar = false
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 := hwy.Load(src[srcIdx:])
					v1 := hwy.Load(src[srcIdx+numLanes:])
					hwy.StoreFull(v0, dst[dstIdx:])
					hwy.StoreFull(v1, dst[dstIdx+numLanes:])
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*4:
			useScalar = false
			var v0, v1, v2, v3 hwy.Vec[hwy.Float16]
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = hwy.Load(src[srcIdx:])
					v1 = hwy.Load(src[srcIdx+numLanes:])
					v2 = hwy.Load(src[srcIdx+numLanes*2:])
					v3 = hwy.Load(src[srcIdx+numLanes*3:])
					hwy.StoreFull(v0, dst[dstIdx:])
					hwy.StoreFull(v1, dst[dstIdx+numLanes:])
					hwy.StoreFull(v2, dst[dstIdx+numLanes*2:])
					hwy.StoreFull(v3, dst[dstIdx+numLanes*3:])
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		}
	}
	if useScalar {
		for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
			srcIdx := srcStartRowIdx + srcColStart + stripColIdx
			for range contractingRows {
				copy(dst[dstIdx:], src[srcIdx:srcIdx+kernelCols])
				dstIdx += kernelCols
				srcIdx += srcRowStride
			}
		}
	}
	validCols := numCols - fullStripsCol
	if validCols == 0 {
		return
	}
	srcIdx := srcStartRowIdx + srcColStart + fullStripsCol
	for range contractingRows {
		copy(dst[dstIdx:], src[srcIdx:srcIdx+validCols])
		dstIdx += validCols
		for range kernelCols - validCols {
			dst[dstIdx] = hwy.Float32ToFloat16(0)
			dstIdx++
		}
	}
}

func BasePackRHS_neon_BFloat16(src []hwy.BFloat16, dst []hwy.BFloat16, srcRowStart int, srcColStart int, srcRowStride int, contractingRows int, numCols int, kernelCols int) {
	dstIdx := 0
	numFullStrips := numCols / kernelCols
	fullStripsCol := numFullStrips * kernelCols
	srcStartRowIdx := srcRowStart * srcRowStride
	useScalar := true
	if hwy.CurrentLevel() != hwy.DispatchScalar {
		numLanes := 8
		switch {
		case kernelCols == numLanes:
			useScalar = false
			var v0 hwy.Vec[hwy.BFloat16]
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = hwy.Load(src[srcIdx:])
					hwy.StoreFull(v0, dst[dstIdx:])
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*2:
			useScalar = false
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 := hwy.Load(src[srcIdx:])
					v1 := hwy.Load(src[srcIdx+numLanes:])
					hwy.StoreFull(v0, dst[dstIdx:])
					hwy.StoreFull(v1, dst[dstIdx+numLanes:])
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*4:
			useScalar = false
			var v0, v1, v2, v3 hwy.Vec[hwy.BFloat16]
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = hwy.Load(src[srcIdx:])
					v1 = hwy.Load(src[srcIdx+numLanes:])
					v2 = hwy.Load(src[srcIdx+numLanes*2:])
					v3 = hwy.Load(src[srcIdx+numLanes*3:])
					hwy.StoreFull(v0, dst[dstIdx:])
					hwy.StoreFull(v1, dst[dstIdx+numLanes:])
					hwy.StoreFull(v2, dst[dstIdx+numLanes*2:])
					hwy.StoreFull(v3, dst[dstIdx+numLanes*3:])
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		}
	}
	if useScalar {
		for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
			srcIdx := srcStartRowIdx + srcColStart + stripColIdx
			for range contractingRows {
				copy(dst[dstIdx:], src[srcIdx:srcIdx+kernelCols])
				dstIdx += kernelCols
				srcIdx += srcRowStride
			}
		}
	}
	validCols := numCols - fullStripsCol
	if validCols == 0 {
		return
	}
	srcIdx := srcStartRowIdx + srcColStart + fullStripsCol
	for range contractingRows {
		copy(dst[dstIdx:], src[srcIdx:srcIdx+validCols])
		dstIdx += validCols
		for range kernelCols - validCols {
			dst[dstIdx] = hwy.Float32ToBFloat16(0)
			dstIdx++
		}
	}
}

func BasePackRHS_neon(src []float32, dst []float32, srcRowStart int, srcColStart int, srcRowStride int, contractingRows int, numCols int, kernelCols int) {
	dstIdx := 0
	numFullStrips := numCols / kernelCols
	fullStripsCol := numFullStrips * kernelCols
	srcStartRowIdx := srcRowStart * srcRowStride
	useScalar := true
	if hwy.CurrentLevel() != hwy.DispatchScalar {
		numLanes := 4
		switch {
		case kernelCols == numLanes:
			useScalar = false
			var v0 asm.Float32x4
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&src[srcIdx])))
					v0.Store((*[4]float32)(unsafe.Pointer(&dst[dstIdx])))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*2:
			useScalar = false
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&src[srcIdx])))
					v1 := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&src[srcIdx+numLanes])))
					v0.Store((*[4]float32)(unsafe.Pointer(&dst[dstIdx])))
					v1.Store((*[4]float32)(unsafe.Pointer(&dst[dstIdx+numLanes])))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*4:
			useScalar = false
			var v0, v1, v2, v3 asm.Float32x4
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&src[srcIdx])))
					v1 = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&src[srcIdx+numLanes])))
					v2 = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&src[srcIdx+numLanes*2])))
					v3 = asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&src[srcIdx+numLanes*3])))
					v0.Store((*[4]float32)(unsafe.Pointer(&dst[dstIdx])))
					v1.Store((*[4]float32)(unsafe.Pointer(&dst[dstIdx+numLanes])))
					v2.Store((*[4]float32)(unsafe.Pointer(&dst[dstIdx+numLanes*2])))
					v3.Store((*[4]float32)(unsafe.Pointer(&dst[dstIdx+numLanes*3])))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		}
	}
	if useScalar {
		for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
			srcIdx := srcStartRowIdx + srcColStart + stripColIdx
			for range contractingRows {
				copy(dst[dstIdx:], src[srcIdx:srcIdx+kernelCols])
				dstIdx += kernelCols
				srcIdx += srcRowStride
			}
		}
	}
	validCols := numCols - fullStripsCol
	if validCols == 0 {
		return
	}
	srcIdx := srcStartRowIdx + srcColStart + fullStripsCol
	for range contractingRows {
		copy(dst[dstIdx:], src[srcIdx:srcIdx+validCols])
		dstIdx += validCols
		for range kernelCols - validCols {
			dst[dstIdx] = 0
			dstIdx++
		}
	}
}

func BasePackRHS_neon_Float64(src []float64, dst []float64, srcRowStart int, srcColStart int, srcRowStride int, contractingRows int, numCols int, kernelCols int) {
	dstIdx := 0
	numFullStrips := numCols / kernelCols
	fullStripsCol := numFullStrips * kernelCols
	srcStartRowIdx := srcRowStart * srcRowStride
	useScalar := true
	if hwy.CurrentLevel() != hwy.DispatchScalar {
		numLanes := 2
		switch {
		case kernelCols == numLanes:
			useScalar = false
			var v0 asm.Float64x2
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&src[srcIdx])))
					v0.Store((*[2]float64)(unsafe.Pointer(&dst[dstIdx])))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*2:
			useScalar = false
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&src[srcIdx])))
					v1 := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&src[srcIdx+numLanes])))
					v0.Store((*[2]float64)(unsafe.Pointer(&dst[dstIdx])))
					v1.Store((*[2]float64)(unsafe.Pointer(&dst[dstIdx+numLanes])))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*4:
			useScalar = false
			var v0, v1, v2, v3 asm.Float64x2
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&src[srcIdx])))
					v1 = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&src[srcIdx+numLanes])))
					v2 = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&src[srcIdx+numLanes*2])))
					v3 = asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&src[srcIdx+numLanes*3])))
					v0.Store((*[2]float64)(unsafe.Pointer(&dst[dstIdx])))
					v1.Store((*[2]float64)(unsafe.Pointer(&dst[dstIdx+numLanes])))
					v2.Store((*[2]float64)(unsafe.Pointer(&dst[dstIdx+numLanes*2])))
					v3.Store((*[2]float64)(unsafe.Pointer(&dst[dstIdx+numLanes*3])))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		}
	}
	if useScalar {
		for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
			srcIdx := srcStartRowIdx + srcColStart + stripColIdx
			for range contractingRows {
				copy(dst[dstIdx:], src[srcIdx:srcIdx+kernelCols])
				dstIdx += kernelCols
				srcIdx += srcRowStride
			}
		}
	}
	validCols := numCols - fullStripsCol
	if validCols == 0 {
		return
	}
	srcIdx := srcStartRowIdx + srcColStart + fullStripsCol
	for range contractingRows {
		copy(dst[dstIdx:], src[srcIdx:srcIdx+validCols])
		dstIdx += validCols
		for range kernelCols - validCols {
			dst[dstIdx] = 0
			dstIdx++
		}
	}
}

func BaseApplyPackedOutput_neon_Float16(packedOutput []hwy.Float16, output []hwy.Float16, alpha hwy.Float16, beta hwy.Float16, packedOutputRowStride int, rowOffset int, colOffset int, outputRowStride int, height int, width int) {
	alphaVec := hwy.Set[hwy.Float16](alpha)
	betaVec := hwy.Set[hwy.Float16](beta)
	for r := range height {
		packedIdx := r * packedOutputRowStride
		outputIdx := (rowOffset+r)*outputRowStride + colOffset
		c := 0
		if hwy.CurrentLevel() != hwy.DispatchScalar {
			numLanes := 8
			for ; c+numLanes <= width; c += numLanes {
				packedVal := hwy.Load(packedOutput[packedIdx:])
				outputVal := hwy.Load(output[outputIdx:])
				newVal := hwy.FMAF16(alphaVec, packedVal, hwy.MulF16(betaVec, outputVal))
				hwy.StoreFull(newVal, output[outputIdx:])
				packedIdx += numLanes
				outputIdx += numLanes
			}
		}
		for ; c < width; c++ {
			val := packedOutput[packedIdx]
			output[outputIdx] = hwy.Float32ToFloat16(beta.Float32()*output[outputIdx].Float32() + alpha.Float32()*val.Float32())
			packedIdx++
			outputIdx++
		}
	}
}

func BaseApplyPackedOutput_neon_BFloat16(packedOutput []hwy.BFloat16, output []hwy.BFloat16, alpha hwy.BFloat16, beta hwy.BFloat16, packedOutputRowStride int, rowOffset int, colOffset int, outputRowStride int, height int, width int) {
	alphaVec := hwy.Set[hwy.BFloat16](alpha)
	betaVec := hwy.Set[hwy.BFloat16](beta)
	for r := range height {
		packedIdx := r * packedOutputRowStride
		outputIdx := (rowOffset+r)*outputRowStride + colOffset
		c := 0
		if hwy.CurrentLevel() != hwy.DispatchScalar {
			numLanes := 8
			for ; c+numLanes <= width; c += numLanes {
				packedVal := hwy.Load(packedOutput[packedIdx:])
				outputVal := hwy.Load(output[outputIdx:])
				newVal := hwy.FMABF16(alphaVec, packedVal, hwy.MulBF16(betaVec, outputVal))
				hwy.StoreFull(newVal, output[outputIdx:])
				packedIdx += numLanes
				outputIdx += numLanes
			}
		}
		for ; c < width; c++ {
			val := packedOutput[packedIdx]
			output[outputIdx] = hwy.Float32ToBFloat16(beta.Float32()*output[outputIdx].Float32() + alpha.Float32()*val.Float32())
			packedIdx++
			outputIdx++
		}
	}
}

func BaseApplyPackedOutput_neon(packedOutput []float32, output []float32, alpha float32, beta float32, packedOutputRowStride int, rowOffset int, colOffset int, outputRowStride int, height int, width int) {
	alphaVec := asm.BroadcastFloat32x4(alpha)
	betaVec := asm.BroadcastFloat32x4(beta)
	for r := range height {
		packedIdx := r * packedOutputRowStride
		outputIdx := (rowOffset+r)*outputRowStride + colOffset
		c := 0
		if hwy.CurrentLevel() != hwy.DispatchScalar {
			numLanes := 4
			for ; c+numLanes <= width; c += numLanes {
				packedVal := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&packedOutput[packedIdx])))
				outputVal := asm.LoadFloat32x4((*[4]float32)(unsafe.Pointer(&output[outputIdx])))
				newVal := alphaVec.MulAdd(packedVal, betaVec.Mul(outputVal))
				newVal.Store((*[4]float32)(unsafe.Pointer(&output[outputIdx])))
				packedIdx += numLanes
				outputIdx += numLanes
			}
		}
		for ; c < width; c++ {
			val := packedOutput[packedIdx]
			output[outputIdx] = beta*output[outputIdx] + alpha*val
			packedIdx++
			outputIdx++
		}
	}
}

func BaseApplyPackedOutput_neon_Float64(packedOutput []float64, output []float64, alpha float64, beta float64, packedOutputRowStride int, rowOffset int, colOffset int, outputRowStride int, height int, width int) {
	alphaVec := asm.BroadcastFloat64x2(alpha)
	betaVec := asm.BroadcastFloat64x2(beta)
	for r := range height {
		packedIdx := r * packedOutputRowStride
		outputIdx := (rowOffset+r)*outputRowStride + colOffset
		c := 0
		if hwy.CurrentLevel() != hwy.DispatchScalar {
			numLanes := 2
			for ; c+numLanes <= width; c += numLanes {
				packedVal := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&packedOutput[packedIdx])))
				outputVal := asm.LoadFloat64x2((*[2]float64)(unsafe.Pointer(&output[outputIdx])))
				newVal := alphaVec.MulAdd(packedVal, betaVec.Mul(outputVal))
				newVal.Store((*[2]float64)(unsafe.Pointer(&output[outputIdx])))
				packedIdx += numLanes
				outputIdx += numLanes
			}
		}
		for ; c < width; c++ {
			val := packedOutput[packedIdx]
			output[outputIdx] = beta*output[outputIdx] + alpha*val
			packedIdx++
			outputIdx++
		}
	}
}
