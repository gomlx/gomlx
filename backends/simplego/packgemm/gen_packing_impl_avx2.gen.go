// Code generated by github.com/ajroetker/go-highway/cmd/hwygen. DO NOT EDIT.

//go:build amd64 && goexperiment.simd

package packgemm

import (
	"simd/archsimd"
	"unsafe"

	"github.com/ajroetker/go-highway/hwy"
	"github.com/ajroetker/go-highway/hwy/asm"
)

func BasePackRHS_avx2_Float16(src []hwy.Float16, dst []hwy.Float16, srcRowStart int, srcColStart int, srcRowStride int, contractingRows int, numCols int, kernelCols int) {
	dstIdx := 0
	numFullStrips := numCols / kernelCols
	fullStripsCol := numFullStrips * kernelCols
	srcStartRowIdx := srcRowStart * srcRowStride
	useScalar := true
	if hwy.CurrentLevel() != hwy.DispatchScalar {
		numLanes := 8
		switch {
		case kernelCols == numLanes:
			useScalar = false
			var v0 asm.Float16x8AVX2
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx:][0]))
					v0.StorePtr(unsafe.Pointer(&dst[dstIdx:][0]))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*2:
			useScalar = false
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx:][0]))
					v1 := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx+numLanes:][0]))
					v0.StorePtr(unsafe.Pointer(&dst[dstIdx:][0]))
					v1.StorePtr(unsafe.Pointer(&dst[dstIdx+numLanes:][0]))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*4:
			useScalar = false
			var v0, v1, v2, v3 asm.Float16x8AVX2
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx:][0]))
					v1 = asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx+numLanes:][0]))
					v2 = asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx+numLanes*2:][0]))
					v3 = asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx+numLanes*3:][0]))
					v0.StorePtr(unsafe.Pointer(&dst[dstIdx:][0]))
					v1.StorePtr(unsafe.Pointer(&dst[dstIdx+numLanes:][0]))
					v2.StorePtr(unsafe.Pointer(&dst[dstIdx+numLanes*2:][0]))
					v3.StorePtr(unsafe.Pointer(&dst[dstIdx+numLanes*3:][0]))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		}
	}
	if useScalar {
		for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
			srcIdx := srcStartRowIdx + srcColStart + stripColIdx
			for range contractingRows {
				copy(dst[dstIdx:], src[srcIdx:srcIdx+kernelCols])
				dstIdx += kernelCols
				srcIdx += srcRowStride
			}
		}
	}
	validCols := numCols - fullStripsCol
	if validCols == 0 {
		return
	}
	srcIdx := srcStartRowIdx + srcColStart + fullStripsCol
	for range contractingRows {
		copy(dst[dstIdx:], src[srcIdx:srcIdx+validCols])
		dstIdx += validCols
		for range kernelCols - validCols {
			dst[dstIdx] = hwy.Float32ToFloat16(0)
			dstIdx++
		}
	}
}

func BasePackRHS_avx2_BFloat16(src []hwy.BFloat16, dst []hwy.BFloat16, srcRowStart int, srcColStart int, srcRowStride int, contractingRows int, numCols int, kernelCols int) {
	dstIdx := 0
	numFullStrips := numCols / kernelCols
	fullStripsCol := numFullStrips * kernelCols
	srcStartRowIdx := srcRowStart * srcRowStride
	useScalar := true
	if hwy.CurrentLevel() != hwy.DispatchScalar {
		numLanes := 8
		switch {
		case kernelCols == numLanes:
			useScalar = false
			var v0 asm.BFloat16x8AVX2
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx:][0]))
					v0.StorePtr(unsafe.Pointer(&dst[dstIdx:][0]))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*2:
			useScalar = false
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx:][0]))
					v1 := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx+numLanes:][0]))
					v0.StorePtr(unsafe.Pointer(&dst[dstIdx:][0]))
					v1.StorePtr(unsafe.Pointer(&dst[dstIdx+numLanes:][0]))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*4:
			useScalar = false
			var v0, v1, v2, v3 asm.BFloat16x8AVX2
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx:][0]))
					v1 = asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx+numLanes:][0]))
					v2 = asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx+numLanes*2:][0]))
					v3 = asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&src[srcIdx+numLanes*3:][0]))
					v0.StorePtr(unsafe.Pointer(&dst[dstIdx:][0]))
					v1.StorePtr(unsafe.Pointer(&dst[dstIdx+numLanes:][0]))
					v2.StorePtr(unsafe.Pointer(&dst[dstIdx+numLanes*2:][0]))
					v3.StorePtr(unsafe.Pointer(&dst[dstIdx+numLanes*3:][0]))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		}
	}
	if useScalar {
		for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
			srcIdx := srcStartRowIdx + srcColStart + stripColIdx
			for range contractingRows {
				copy(dst[dstIdx:], src[srcIdx:srcIdx+kernelCols])
				dstIdx += kernelCols
				srcIdx += srcRowStride
			}
		}
	}
	validCols := numCols - fullStripsCol
	if validCols == 0 {
		return
	}
	srcIdx := srcStartRowIdx + srcColStart + fullStripsCol
	for range contractingRows {
		copy(dst[dstIdx:], src[srcIdx:srcIdx+validCols])
		dstIdx += validCols
		for range kernelCols - validCols {
			dst[dstIdx] = hwy.Float32ToBFloat16(0)
			dstIdx++
		}
	}
}

func BasePackRHS_avx2(src []float32, dst []float32, srcRowStart int, srcColStart int, srcRowStride int, contractingRows int, numCols int, kernelCols int) {
	dstIdx := 0
	numFullStrips := numCols / kernelCols
	fullStripsCol := numFullStrips * kernelCols
	srcStartRowIdx := srcRowStart * srcRowStride
	useScalar := true
	if hwy.CurrentLevel() != hwy.DispatchScalar {
		numLanes := 8
		switch {
		case kernelCols == numLanes:
			useScalar = false
			var v0 archsimd.Float32x8
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[srcIdx])))
					v0.Store((*[8]float32)(unsafe.Pointer(&dst[dstIdx])))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*2:
			useScalar = false
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[srcIdx])))
					v1 := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[srcIdx+numLanes])))
					v0.Store((*[8]float32)(unsafe.Pointer(&dst[dstIdx])))
					v1.Store((*[8]float32)(unsafe.Pointer(&dst[dstIdx+numLanes])))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*4:
			useScalar = false
			var v0, v1, v2, v3 archsimd.Float32x8
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[srcIdx])))
					v1 = archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[srcIdx+numLanes])))
					v2 = archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[srcIdx+numLanes*2])))
					v3 = archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&src[srcIdx+numLanes*3])))
					v0.Store((*[8]float32)(unsafe.Pointer(&dst[dstIdx])))
					v1.Store((*[8]float32)(unsafe.Pointer(&dst[dstIdx+numLanes])))
					v2.Store((*[8]float32)(unsafe.Pointer(&dst[dstIdx+numLanes*2])))
					v3.Store((*[8]float32)(unsafe.Pointer(&dst[dstIdx+numLanes*3])))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		}
	}
	if useScalar {
		for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
			srcIdx := srcStartRowIdx + srcColStart + stripColIdx
			for range contractingRows {
				copy(dst[dstIdx:], src[srcIdx:srcIdx+kernelCols])
				dstIdx += kernelCols
				srcIdx += srcRowStride
			}
		}
	}
	validCols := numCols - fullStripsCol
	if validCols == 0 {
		return
	}
	srcIdx := srcStartRowIdx + srcColStart + fullStripsCol
	for range contractingRows {
		copy(dst[dstIdx:], src[srcIdx:srcIdx+validCols])
		dstIdx += validCols
		for range kernelCols - validCols {
			dst[dstIdx] = 0
			dstIdx++
		}
	}
}

func BasePackRHS_avx2_Float64(src []float64, dst []float64, srcRowStart int, srcColStart int, srcRowStride int, contractingRows int, numCols int, kernelCols int) {
	dstIdx := 0
	numFullStrips := numCols / kernelCols
	fullStripsCol := numFullStrips * kernelCols
	srcStartRowIdx := srcRowStart * srcRowStride
	useScalar := true
	if hwy.CurrentLevel() != hwy.DispatchScalar {
		numLanes := 4
		switch {
		case kernelCols == numLanes:
			useScalar = false
			var v0 archsimd.Float64x4
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&src[srcIdx])))
					v0.Store((*[4]float64)(unsafe.Pointer(&dst[dstIdx])))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*2:
			useScalar = false
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&src[srcIdx])))
					v1 := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&src[srcIdx+numLanes])))
					v0.Store((*[4]float64)(unsafe.Pointer(&dst[dstIdx])))
					v1.Store((*[4]float64)(unsafe.Pointer(&dst[dstIdx+numLanes])))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		case kernelCols == numLanes*4:
			useScalar = false
			var v0, v1, v2, v3 archsimd.Float64x4
			for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
				srcIdx := srcStartRowIdx + srcColStart + stripColIdx
				for range contractingRows {
					v0 = archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&src[srcIdx])))
					v1 = archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&src[srcIdx+numLanes])))
					v2 = archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&src[srcIdx+numLanes*2])))
					v3 = archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&src[srcIdx+numLanes*3])))
					v0.Store((*[4]float64)(unsafe.Pointer(&dst[dstIdx])))
					v1.Store((*[4]float64)(unsafe.Pointer(&dst[dstIdx+numLanes])))
					v2.Store((*[4]float64)(unsafe.Pointer(&dst[dstIdx+numLanes*2])))
					v3.Store((*[4]float64)(unsafe.Pointer(&dst[dstIdx+numLanes*3])))
					dstIdx += kernelCols
					srcIdx += srcRowStride
				}
			}
		}
	}
	if useScalar {
		for stripColIdx := 0; stripColIdx < fullStripsCol; stripColIdx += kernelCols {
			srcIdx := srcStartRowIdx + srcColStart + stripColIdx
			for range contractingRows {
				copy(dst[dstIdx:], src[srcIdx:srcIdx+kernelCols])
				dstIdx += kernelCols
				srcIdx += srcRowStride
			}
		}
	}
	validCols := numCols - fullStripsCol
	if validCols == 0 {
		return
	}
	srcIdx := srcStartRowIdx + srcColStart + fullStripsCol
	for range contractingRows {
		copy(dst[dstIdx:], src[srcIdx:srcIdx+validCols])
		dstIdx += validCols
		for range kernelCols - validCols {
			dst[dstIdx] = 0
			dstIdx++
		}
	}
}

func BaseApplyPackedOutput_avx2_Float16(packedOutput []hwy.Float16, output []hwy.Float16, alpha hwy.Float16, beta hwy.Float16, packedOutputRowStride int, rowOffset int, colOffset int, outputRowStride int, height int, width int) {
	alphaVec := asm.BroadcastFloat16x8AVX2(uint16(alpha))
	betaVec := asm.BroadcastFloat16x8AVX2(uint16(beta))
	for r := range height {
		packedIdx := r * packedOutputRowStride
		outputIdx := (rowOffset+r)*outputRowStride + colOffset
		c := 0
		if hwy.CurrentLevel() != hwy.DispatchScalar {
			numLanes := 8
			for ; c+numLanes <= width; c += numLanes {
				packedVal := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&packedOutput[packedIdx:][0]))
				outputVal := asm.LoadFloat16x8AVX2Ptr(unsafe.Pointer(&output[outputIdx:][0]))
				newVal := alphaVec.MulAdd(packedVal, betaVec.Mul(outputVal))
				newVal.StorePtr(unsafe.Pointer(&output[outputIdx:][0]))
				packedIdx += numLanes
				outputIdx += numLanes
			}
		}
		for ; c < width; c++ {
			val := packedOutput[packedIdx]
			output[outputIdx] = hwy.Float32ToFloat16(beta.Float32()*output[outputIdx].Float32() + alpha.Float32()*val.Float32())
			packedIdx++
			outputIdx++
		}
	}
}

func BaseApplyPackedOutput_avx2_BFloat16(packedOutput []hwy.BFloat16, output []hwy.BFloat16, alpha hwy.BFloat16, beta hwy.BFloat16, packedOutputRowStride int, rowOffset int, colOffset int, outputRowStride int, height int, width int) {
	alphaVec := asm.BroadcastBFloat16x8AVX2(uint16(alpha))
	betaVec := asm.BroadcastBFloat16x8AVX2(uint16(beta))
	for r := range height {
		packedIdx := r * packedOutputRowStride
		outputIdx := (rowOffset+r)*outputRowStride + colOffset
		c := 0
		if hwy.CurrentLevel() != hwy.DispatchScalar {
			numLanes := 8
			for ; c+numLanes <= width; c += numLanes {
				packedVal := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&packedOutput[packedIdx:][0]))
				outputVal := asm.LoadBFloat16x8AVX2Ptr(unsafe.Pointer(&output[outputIdx:][0]))
				newVal := alphaVec.MulAdd(packedVal, betaVec.Mul(outputVal))
				newVal.StorePtr(unsafe.Pointer(&output[outputIdx:][0]))
				packedIdx += numLanes
				outputIdx += numLanes
			}
		}
		for ; c < width; c++ {
			val := packedOutput[packedIdx]
			output[outputIdx] = hwy.Float32ToBFloat16(beta.Float32()*output[outputIdx].Float32() + alpha.Float32()*val.Float32())
			packedIdx++
			outputIdx++
		}
	}
}

func BaseApplyPackedOutput_avx2(packedOutput []float32, output []float32, alpha float32, beta float32, packedOutputRowStride int, rowOffset int, colOffset int, outputRowStride int, height int, width int) {
	alphaVec := archsimd.BroadcastFloat32x8(alpha)
	betaVec := archsimd.BroadcastFloat32x8(beta)
	for r := range height {
		packedIdx := r * packedOutputRowStride
		outputIdx := (rowOffset+r)*outputRowStride + colOffset
		c := 0
		if hwy.CurrentLevel() != hwy.DispatchScalar {
			numLanes := 8
			for ; c+numLanes <= width; c += numLanes {
				packedVal := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&packedOutput[packedIdx])))
				outputVal := archsimd.LoadFloat32x8((*[8]float32)(unsafe.Pointer(&output[outputIdx])))
				newVal := alphaVec.MulAdd(packedVal, betaVec.Mul(outputVal))
				newVal.Store((*[8]float32)(unsafe.Pointer(&output[outputIdx])))
				packedIdx += numLanes
				outputIdx += numLanes
			}
		}
		for ; c < width; c++ {
			val := packedOutput[packedIdx]
			output[outputIdx] = beta*output[outputIdx] + alpha*val
			packedIdx++
			outputIdx++
		}
	}
}

func BaseApplyPackedOutput_avx2_Float64(packedOutput []float64, output []float64, alpha float64, beta float64, packedOutputRowStride int, rowOffset int, colOffset int, outputRowStride int, height int, width int) {
	alphaVec := archsimd.BroadcastFloat64x4(alpha)
	betaVec := archsimd.BroadcastFloat64x4(beta)
	for r := range height {
		packedIdx := r * packedOutputRowStride
		outputIdx := (rowOffset+r)*outputRowStride + colOffset
		c := 0
		if hwy.CurrentLevel() != hwy.DispatchScalar {
			numLanes := 4
			for ; c+numLanes <= width; c += numLanes {
				packedVal := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&packedOutput[packedIdx])))
				outputVal := archsimd.LoadFloat64x4((*[4]float64)(unsafe.Pointer(&output[outputIdx])))
				newVal := alphaVec.MulAdd(packedVal, betaVec.Mul(outputVal))
				newVal.Store((*[4]float64)(unsafe.Pointer(&output[outputIdx])))
				packedIdx += numLanes
				outputIdx += numLanes
			}
		}
		for ; c < width; c++ {
			val := packedOutput[packedIdx]
			output[outputIdx] = beta*output[outputIdx] + alpha*val
			packedIdx++
			outputIdx++
		}
	}
}
